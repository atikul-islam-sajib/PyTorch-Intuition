{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TEA3dOWOyjrG",
        "outputId": "fe2d970a-0503-425f-fa4e-840d781e2ab6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.5486], grad_fn=<PowBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Simple Discriminator (Critic) Model\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(784, 512),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.Linear(512, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Simple Generator Model\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(100, 512),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(512, 784),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        return self.model(z)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def gradient_penalty(D, real, fake, device):\n",
        "  alpha = torch.randn(real.size(0), 1).to(device)\n",
        "  interpolated_samples = alpha * real + ((1-alpha) * fake)\n",
        "  interpolated_samples = interpolated_samples.requires_grad_(True)\n",
        "\n",
        "  d_interploated = D(interpolated_samples)\n",
        "\n",
        "  gradients = torch.autograd.grad(\n",
        "      outputs=d_interploated,\n",
        "      inputs=interpolated_samples,\n",
        "      grad_outputs=torch.ones_like(d_interploated),\n",
        "      retain_graph=True,\n",
        "      create_graph=True\n",
        "\n",
        "  )[0] # As it return in tuple\n",
        "\n",
        "  gradients = torch.norm(gradients, 2, dim = 1)\n",
        "  penalty = (gradients - 1)**2\n",
        "\n",
        "  return penalty\n",
        "\n",
        "# # Gradient Penalty Function\n",
        "# def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "#     batch_size = real_samples.size(0)\n",
        "#     # Random weight term for interpolation between real and fake samples\n",
        "#     alpha = torch.rand(batch_size, 1, device=real_samples.device)\n",
        "#     # Interpolate between real and fake samples\n",
        "#     interpolates = alpha * real_samples + ((1 - alpha) * fake_samples)\n",
        "#     interpolates.requires_grad_(True)\n",
        "\n",
        "#     d_interpolates = D(interpolates)\n",
        "\n",
        "#     gradients = torch.autograd.grad(\n",
        "#         outputs=d_interpolates,\n",
        "#         inputs=interpolates,\n",
        "#         grad_outputs=torch.ones_like(d_interpolates),\n",
        "#         create_graph=True,\n",
        "#         retain_graph=True\n",
        "#     )[0]\n",
        "\n",
        "#     gradients = gradients.view(batch_size, -1)\n",
        "#     gradient_norm = torch.sqrt(torch.sum(gradients ** 2, dim=1))\n",
        "#     gradient_penalty = (gradient_norm - 1) ** 2\n",
        "#     return gradient_penalty\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "D = Discriminator().to(device)\n",
        "G = Generator().to(device)\n",
        "gradient_penalty(D, torch.randn(1, 784), torch.randn(1, 784), device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Create a 2D tensor (matrix)\n",
        "tensor = torch.tensor([[1.0, 2.0, 3.0],\n",
        "                       [4.0, 5.0, 6.0]])\n",
        "\n",
        "# Compute L2 norm along dimension 0\n",
        "norm_dim0 = torch.norm(tensor, p=2, dim=0)\n",
        "print(\"L2 Norm along dimension 0:\", norm_dim0)\n",
        "\n",
        "# Compute L2 norm along dimension 1\n",
        "norm_dim1 = torch.norm(tensor, p=2, dim=1)\n",
        "print(\"L2 Norm along dimension 1:\", norm_dim1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-z7Qh-hy6_v",
        "outputId": "a2226f08-3f6e-4d67-b4ed-2c983ed9e468"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "L2 Norm along dimension 0: tensor([4.1231, 5.3852, 6.7082])\n",
            "L2 Norm along dimension 1: tensor([3.7417, 8.7750])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Conv2D-based Discriminator (Critic) Model for RGB images\n",
        "class ConvDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvDiscriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Gradient Penalty Function\n",
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    batch_size, c, h, w = real_samples.shape\n",
        "\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = torch.rand(batch_size, 1, 1, 1, device=real_samples.device)\n",
        "    # Interpolate between real and fake samples\n",
        "    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n",
        "    interpolates.requires_grad_(True)\n",
        "\n",
        "    # Calculate the discriminator's output on the interpolated samples\n",
        "    d_interpolates = D(interpolates)\n",
        "\n",
        "    # Compute gradients of the discriminator's output with respect to the interpolated samples\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    # Reshape the gradients to (batch_size, num_elements)\n",
        "    gradients = gradients.view(batch_size, -1)\n",
        "    # Compute the L2 norm of the gradients for each sample in the batch\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    # Compute the gradient penalty\n",
        "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "    return gradient_penalty\n",
        "\n",
        "# Sample usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "D = ConvDiscriminator().to(device)\n",
        "\n",
        "real_samples = torch.randn(1, 3, 64, 64, device=device)  # Example real samples (batch_size=5, 3 channels, 64x64 image)\n",
        "fake_samples = torch.randn(1, 3, 64, 64, device=device)  # Example fake samples\n",
        "\n",
        "gradient_penalty = compute_gradient_penalty(D, real_samples, fake_samples)\n",
        "print(\"Gradient Penalty:\", gradient_penalty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DKeRHudo1buQ",
        "outputId": "d7b71b21-6d6a-4e01-a417-b093ae72469c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradient Penalty: tensor(3.4620, grad_fn=<MeanBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the Conv2D-based Discriminator (Critic) Model for RGB images\n",
        "class ConvDiscriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ConvDiscriminator, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(128),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
        "            nn.BatchNorm2d(256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(256, 1, kernel_size=4, stride=1, padding=0)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Gradient Penalty Function\n",
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    batch_size, c, h, w = real_samples.shape\n",
        "\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = torch.rand(batch_size, 3, 64, 64, device=real_samples.device)\n",
        "    # Interpolate between real and fake samples\n",
        "    interpolates = alpha * real_samples + (1 - alpha) * fake_samples\n",
        "    interpolates.requires_grad_(True)\n",
        "\n",
        "    # Calculate the discriminator's output on the interpolated samples\n",
        "    d_interpolates = D(interpolates)\n",
        "\n",
        "    # Compute gradients of the discriminator's output with respect to the interpolated samples\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True,\n",
        "        retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    # Reshape the gradients to (batch_size, num_elements)\n",
        "    print(gradients.size())\n",
        "    # gradients = gradients.view(batch_size, -1)\n",
        "    # Compute the L2 norm of the gradients for each sample in the batch\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    # Compute the gradient penalty\n",
        "    gradient_penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "    return gradients\n",
        "\n",
        "# Sample usage\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "D = ConvDiscriminator().to(device)\n",
        "\n",
        "real_samples = torch.randn(1, 3, 64, 64, device=device)  # Example real samples (batch_size=5, 3 channels, 64x64 image)\n",
        "fake_samples = torch.randn(1, 3, 64, 64, device=device)  # Example fake samples\n",
        "\n",
        "gradient_penalty = compute_gradient_penalty(D, real_samples, fake_samples)\n",
        "print(\"Gradient Penalty:\", gradient_penalty)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LOb_TO-y6unN",
        "outputId": "84e7ba82-8d17-41d0-a7de-885626de0231"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([1, 3, 64, 64])\n",
            "Gradient Penalty: tensor([[[[ 4.4321e-03,  4.1723e-03,  1.7816e-03,  ...,  5.4069e-03,\n",
            "           -3.8142e-03, -8.7445e-04],\n",
            "          [-4.3623e-05, -7.5737e-03,  1.4225e-03,  ..., -1.8151e-03,\n",
            "           -7.5461e-04, -6.7127e-03],\n",
            "          [ 2.1676e-03, -3.4768e-03, -6.8543e-03,  ..., -1.1667e-02,\n",
            "           -5.7196e-03,  3.6098e-04],\n",
            "          ...,\n",
            "          [-4.7825e-03,  4.3432e-04,  1.9166e-02,  ..., -7.1255e-03,\n",
            "            9.5771e-03,  6.7436e-03],\n",
            "          [ 1.3610e-03, -1.8163e-03, -2.8733e-03,  ..., -1.1494e-02,\n",
            "            1.8334e-03, -2.4156e-03],\n",
            "          [ 1.2208e-03, -1.2457e-03, -1.4444e-03,  ..., -3.5302e-04,\n",
            "           -1.2836e-03,  1.9308e-03]],\n",
            "\n",
            "         [[-3.0312e-03,  1.3824e-03, -1.5471e-03,  ..., -1.2368e-03,\n",
            "           -3.0324e-03, -1.2677e-03],\n",
            "          [-6.0652e-03, -1.4605e-03, -1.1349e-02,  ...,  6.0605e-03,\n",
            "            1.4260e-02,  3.4764e-03],\n",
            "          [ 1.9638e-03, -8.1987e-03,  3.1535e-04,  ...,  1.5996e-02,\n",
            "            6.1662e-03, -5.0898e-03],\n",
            "          ...,\n",
            "          [ 9.0652e-03, -5.9597e-03,  3.2347e-04,  ...,  1.1828e-04,\n",
            "           -1.2102e-02,  3.4783e-03],\n",
            "          [-1.6640e-03,  4.1924e-04,  1.3983e-02,  ...,  1.4416e-02,\n",
            "            1.1205e-02, -6.2761e-03],\n",
            "          [-2.8952e-03,  7.7436e-03, -3.1971e-04,  ..., -1.7920e-03,\n",
            "           -1.8746e-04, -2.7877e-03]],\n",
            "\n",
            "         [[ 1.6086e-03, -1.1138e-03, -5.2123e-03,  ...,  3.7971e-03,\n",
            "           -2.5611e-03,  6.2828e-03],\n",
            "          [-1.8159e-03,  2.3411e-03,  1.5780e-02,  ..., -1.8535e-02,\n",
            "           -6.6479e-03,  2.6990e-04],\n",
            "          [ 4.1543e-03,  5.7595e-03,  2.0872e-03,  ...,  2.2103e-03,\n",
            "            1.4993e-02,  5.4623e-04],\n",
            "          ...,\n",
            "          [-4.2782e-03,  1.3076e-02, -3.4060e-03,  ...,  9.2934e-03,\n",
            "            2.3673e-03, -4.9764e-03],\n",
            "          [ 1.6900e-03, -1.7298e-02,  1.0946e-03,  ..., -6.9611e-03,\n",
            "            9.9193e-03,  3.2291e-03],\n",
            "          [ 3.5938e-03,  9.6727e-04, -3.1267e-03,  ..., -6.6063e-03,\n",
            "           -2.5601e-03, -1.2296e-03]]]],\n",
            "       grad_fn=<ConvolutionBackwardBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor([5.0, 2.0], requires_grad=True)\n",
        "y = torch.tensor([2.0, 5.0], requires_grad=True)\n",
        "\n",
        "# Define the equation\n",
        "equation = 3.0 * X ** 2.0 + 2.0 * y * 2.0\n",
        "\n",
        "# Compute gradients\n",
        "gradients = torch.autograd.grad(\n",
        "    outputs=equation,\n",
        "    inputs=[X, y],\n",
        "    grad_outputs=torch.ones_like(equation),  # Ensure grad_outputs matches the shape of `equation`\n",
        "    create_graph=True,\n",
        "    retain_graph=True\n",
        ")\n",
        "\n",
        "print(gradients)\n",
        "\n",
        "print(\"Gradients with respect to X:\", gradients[0])\n",
        "print(\"Gradients with respect to y:\", gradients[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-96Dg52q6x9a",
        "outputId": "958fa182-844a-488f-fe3f-f825c9aac161"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor([30., 12.], grad_fn=<MulBackward0>), tensor([4., 4.]))\n",
            "Gradients with respect to X: tensor([30., 12.], grad_fn=<MulBackward0>)\n",
            "Gradients with respect to y: tensor([4., 4.])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7JlDcFcARTy"
      },
      "execution_count": 81,
      "outputs": []
    }
  ]
}