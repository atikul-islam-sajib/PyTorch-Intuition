{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FeWTh7eWIr6W"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Encoder(nn.Module):\n",
        "  def __init__(self, in_channels = None, out_channels = None):\n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.Conv2d(self.in_channels, self.out_channels, 3, 1, 1),\n",
        "        nn.ReLU(inplace = True),\n",
        "        nn.Conv2d(self.out_channels, self.out_channels, 3, 1, 1),\n",
        "        nn.BatchNorm2d(self.out_channels),\n",
        "        nn.ReLU(inplace=True)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.model(x)"
      ],
      "metadata": {
        "id": "8ISk-H8xIyX2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.randn(64, 3, 256, 256)\n",
        "encoder = Encoder(in_channels=3, out_channels=64)\n",
        "\n",
        "encoder(data).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3_OhOxTJp6x",
        "outputId": "195859c6-d68d-4bb7-a356-2e8d5f600d69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 64, 256, 256])"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Decoder(nn.Module):\n",
        "  def __init__(self, in_channels = None, out_channels = None):\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    self.model = nn.Sequential(\n",
        "        nn.ConvTranspose2d(self.in_channels, self.out_channels, 2, 2, 0)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    x = self.model(x)\n",
        "    return x"
      ],
      "metadata": {
        "id": "XrAhL7reJ5v0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = torch.randn(64, 1024, 16, 16)\n",
        "data2 = torch.randn(64, 512, 32, 32)\n",
        "\n",
        "decoder = Decoder(in_channels=1024, out_channels=512)\n",
        "\n",
        "decoder(data1, data2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30dIZ4qAKizA",
        "outputId": "a047de8a-e5e3-4e39-af12-8d7966efb2be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1024, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionBlock(nn.Module):\n",
        "  def __init__(self, in_channels = None, out_channels = None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.in_channels = in_channels\n",
        "    self.out_channels = out_channels\n",
        "\n",
        "    self.W_gate = nn.Sequential(\n",
        "        nn.Conv2d(self.in_channels, self.out_channels, 1, 1, 0, bias=True),\n",
        "        nn.BatchNorm2d(self.out_channels)\n",
        "    )\n",
        "\n",
        "    self.W_x = nn.Sequential(\n",
        "        nn.Conv2d(self.in_channels, self.out_channels, 1, 1, 0, bias=True),\n",
        "        nn.BatchNorm2d(self.out_channels)\n",
        "    )\n",
        "\n",
        "    self.psi = nn.Sequential(\n",
        "        nn.Conv2d(self.in_channels*2, 1, 1, 1, 0, bias=True),\n",
        "        nn.BatchNorm2d(1),\n",
        "        nn.Sigmoid()\n",
        "    )\n",
        "\n",
        "    self.relu = nn.ReLU(inplace = True)\n",
        "\n",
        "  def forward(self,x, skip_info):\n",
        "    x = self.W_gate(x)\n",
        "    skip = self.W_x(skip_info)\n",
        "\n",
        "    concat = torch.cat((x, skip), dim = 1)\n",
        "    concat = self.relu(concat)\n",
        "\n",
        "    out = self.psi(concat)\n",
        "\n",
        "    resampled = out * skip\n",
        "\n",
        "    return resampled"
      ],
      "metadata": {
        "id": "scIxFtWsLDUQ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data1 = torch.randn(64, 512, 16, 16)      # 64, 512, 8, 8\n",
        "data2 = torch.randn(64, 512, 16, 16)   # 64, 512, 8, 8 # skip\n",
        "\n",
        "# '''\n",
        "# 64, 1024, 8, 8 #\n",
        "\n",
        "# 64, 512, 16, 16 # skip_info\n",
        "# '''\n",
        "\n",
        "attention = AttentionBlock(in_channels=512, out_channels=512)\n",
        "\n",
        "attention(data1, data2).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9h7XnZjrNO2r",
        "outputId": "244dc7bb-51b2-4174-8287-bc5f613776bb"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 512, 16, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from math import e\n",
        "class AttentionUNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "\n",
        "    self.encoder_block1 = Encoder(in_channels=3, out_channels=64)\n",
        "    self.encoder_block2 = Encoder(in_channels=64, out_channels=128)\n",
        "    self.encoder_block3 = Encoder(in_channels=128, out_channels=256)\n",
        "    self.encoder_block4 = Encoder(in_channels=256, out_channels=512)\n",
        "    self.encoder_block5 = Encoder(in_channels=512, out_channels=1024)\n",
        "\n",
        "    self.intermiadte_block1 = Encoder(in_channels=1024, out_channels=512)\n",
        "    self.intermiadte_block2 = Encoder(in_channels=512, out_channels=256)\n",
        "    self.intermiadte_block3 = Encoder(in_channels=256, out_channels=128)\n",
        "    self.intermiadte_block4 = Encoder(in_channels=128, out_channels=64)\n",
        "\n",
        "    self.decoder_block1 = Decoder(in_channels=1024, out_channels=512)\n",
        "    self.decoder_block2 = Decoder(in_channels=512, out_channels=256)\n",
        "    self.decoder_block3 = Decoder(in_channels=256, out_channels=128)\n",
        "    self.decoder_block4 = Decoder(in_channels=128, out_channels=64)\n",
        "\n",
        "    self.maxpool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "    self.attention_block1 = AttentionBlock(in_channels=512, out_channels=512)\n",
        "    self.attention_block2 = AttentionBlock(in_channels=256, out_channels=256)\n",
        "    self.attention_block3 = AttentionBlock(in_channels=128, out_channels=128)\n",
        "    self.attention_block4 = AttentionBlock(in_channels=64, out_channels=64)\n",
        "\n",
        "\n",
        "    self.final = nn.Conv2d(64, 1, kernel_size=1, stride=1, padding=0)\n",
        "\n",
        "\n",
        "  def forward(self, x):\n",
        "    e1 = self.encoder_block1(x)\n",
        "    e1_out = self.maxpool(e1)\n",
        "\n",
        "    e2 = self.encoder_block2(e1_out)\n",
        "    e2_out = self.maxpool(e2)\n",
        "\n",
        "    e3 = self.encoder_block3(e2_out)\n",
        "    e3_out = self.maxpool(e3)\n",
        "\n",
        "    e4 = self.encoder_block4(e3_out)    # skip_info\n",
        "    e4_out = self.maxpool(e4)\n",
        "\n",
        "    e5 = self.encoder_block5(e4_out)    # out\n",
        "\n",
        "    up1 = self.decoder_block1(e5)\n",
        "\n",
        "\n",
        "    att1 = self.attention_block1(up1, e4)\n",
        "    att1 = torch.cat((att1, e4), dim = 1)\n",
        "\n",
        "\n",
        "    # de1 = self.decoder_block1(att1)\n",
        "    de1_out = self.intermiadte_block1(att1)\n",
        "\n",
        "    up2 = self.decoder_block2(de1_out)\n",
        "    att2 = self.attention_block2(up2, e3)\n",
        "    att2 = torch.cat((att2, e3), dim = 1)\n",
        "\n",
        "    de2_out = self.intermiadte_block2(att2)\n",
        "\n",
        "\n",
        "    up3 = self.decoder_block3(de2_out)\n",
        "    att3 = self.attention_block3(up3, e2)\n",
        "    att3 = torch.cat((att3, e2), dim = 1)\n",
        "\n",
        "    de3_out = self.intermiadte_block3(att3)\n",
        "\n",
        "    up4 = self.decoder_block4(de3_out)\n",
        "    att4 = self.attention_block4(up4, e1)\n",
        "    att4 = torch.cat((att4, e1), dim = 1)\n",
        "\n",
        "    de4_out = self.intermiadte_block4(att4)\n",
        "\n",
        "\n",
        "    out = self.final(de4_out)\n",
        "\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "GsgiarO2Oq7M"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = torch.randn(64, 3, 128, 128)\n",
        "model = AttentionUNet()\n",
        "model(data).shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CBEnOYtzRCt6",
        "outputId": "90bc2a84-56e1-46d7-bd10-601e0de45275"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([64, 1, 128, 128])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}