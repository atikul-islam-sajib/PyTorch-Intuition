{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTrmjYi1R8KN"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Self-Attention Convolutional Block"
      ],
      "metadata": {
        "id": "BInxL_IeR9P1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttentionConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, heads=8):\n",
        "        super(SelfAttentionConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.attention = SelfAttention(out_channels, heads)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x)\n",
        "        conv_out = self.bn(conv_out)\n",
        "        attn_out = self.attention(conv_out)\n",
        "        return self.relu(attn_out)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels, heads=8):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = channels ** -0.5\n",
        "\n",
        "        self.query = nn.Conv2d(channels, channels, 1)\n",
        "        self.key = nn.Conv2d(channels, channels, 1)\n",
        "        self.value = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "        q = self.query(x).view(b, self.heads, c // self.heads, h * w)\n",
        "        k = self.key(x).view(b, self.heads, c // self.heads, h * w)\n",
        "        v = self.value(x).view(b, self.heads, c // self.heads, h * w)\n",
        "\n",
        "        attention = torch.einsum('bhcn,bhck->bhnk', q, k) * self.scale\n",
        "        attention = torch.softmax(attention, dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhnk,bhcv->bhcv', attention, v)\n",
        "        out = out.view(b, c, h, w)\n",
        "\n",
        "        out = self.out_conv(out)\n",
        "        return out\n",
        "\n",
        "# Example usage\n",
        "if __name__ == \"__main__\":\n",
        "    model = SelfAttentionConvBlock(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, heads=8)\n",
        "    input_tensor = torch.randn(16, 3, 32, 32)  # Example input (batch_size, channels, height, width)\n",
        "    output = model(input_tensor)\n",
        "    print(output.shape)  # Expected output shape: (16, 64, 32, 32)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u0R_5M4pR9cZ",
        "outputId": "621327d1-2a7c-4406-d21b-940ead755b76"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Break down the code"
      ],
      "metadata": {
        "id": "WzBOuH2wSH7a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SelfAttentionConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, heads=8):\n",
        "        super(SelfAttentionConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x)\n",
        "        conv_out = self.bn(conv_out)\n",
        "\n",
        "        return conv_out\n",
        "\n",
        "model = SelfAttentionConvBlock(in_channels=3, out_channels=64, kernel_size=3, stride=1, padding=1, heads=8)\n",
        "input_tensor = torch.randn(16, 3, 32, 32)  # Example input (batch_size, channels, height, width)\n",
        "output = model(input_tensor)\n",
        "\n",
        "print(output.size())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WgFMWj4CR_gP",
        "outputId": "8b1f4358-6057-403b-f861-26d28d6dda9d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([16, 64, 32, 32])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "A = nn.Conv2d(32*2, 32*2, 1)\n",
        "\n",
        "A(torch.randn(1, 64, 32, 32)).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kcq62Q-SMOi",
        "outputId": "d2b13480-7c0c-4fce-cacb-0a1e15249c6d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 32, 32])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "B = A(torch.randn(1, 64, 32, 32))\n",
        "B.view(B.size(0), B.size(1), -1).size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0tcdIppETwmG",
        "outputId": "5f0f624f-a649-4d72-bba4-50bf100d3da6"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 64, 1024])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(1, 8, 64//8, 1024)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6PHKb58T_Kj",
        "outputId": "20f8d536-2b2e-454e-c1b2-a6b7cf14be5b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 8, 8, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, channels, heads=8):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.heads = heads\n",
        "        self.scale = channels ** -0.5\n",
        "\n",
        "        self.query = nn.Conv2d(channels, channels, 1)\n",
        "        self.key = nn.Conv2d(channels, channels, 1)\n",
        "        self.value = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "        self.out_conv = nn.Conv2d(channels, channels, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c, h, w = x.size()\n",
        "        q = self.query(x).view(b, self.heads, c // self.heads, h * w)\n",
        "        k = self.key(x).view(b, self.heads, c // self.heads, h * w)\n",
        "        v = self.value(x).view(b, self.heads, c // self.heads, h * w)\n",
        "\n",
        "        attention = torch.einsum('bhcn,bhck->bhnk', q, k) * self.scale\n",
        "        attention = torch.softmax(attention, dim=-1)\n",
        "\n",
        "        out = torch.einsum('bhnk,bhcv->bhcv', attention, v)\n",
        "        out = out.view(b, c, h, w)\n",
        "\n",
        "        out = self.out_conv(out)\n",
        "        return out\n",
        "\n",
        "class SelfAttentionConvBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, heads=8):\n",
        "        super(SelfAttentionConvBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
        "        self.bn = nn.BatchNorm2d(out_channels)\n",
        "        self.attention = SelfAttention(out_channels, heads)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        conv_out = self.conv(x)\n",
        "        conv_out = self.bn(conv_out)\n",
        "        attn_out = self.attention(conv_out)\n",
        "        return self.relu(attn_out)\n",
        "\n",
        "class MNISTSelfAttentionModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MNISTSelfAttentionModel, self).__init__()\n",
        "\n",
        "        self.layer1 = SelfAttentionConvBlock(in_channels=1, out_channels=32, kernel_size=3, stride=1, padding=1, heads=8)\n",
        "        self.layer2 = SelfAttentionConvBlock(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, heads=8)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.layer1(x)\n",
        "        x = self.pool(x)  # Reduces size to 14x14\n",
        "        x = self.layer2(x)\n",
        "        x = self.pool(x)  # Reduces size to 7x7\n",
        "        x = x.view(-1, 64 * 7 * 7)  # Flatten the tensor\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Check for GPU\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define Hyperparameters\n",
        "batch_size = 64\n",
        "learning_rate = 0.001\n",
        "num_epochs = 5\n",
        "\n",
        "# Load MNIST Data\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = MNISTSelfAttentionModel().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')\n",
        "\n",
        "# Testing the model\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Test Accuracy of the model on the 10000 test images: {100 * correct / total:.2f}%')\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), 'mnist_self_attention_model.pth')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLwX64EeTCxm",
        "outputId": "64bd2f8d-95ca-4cfd-bfed-28aeffa0ae18"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Epoch [1/5], Loss: 0.9409\n",
            "Epoch [2/5], Loss: 0.0818\n",
            "Epoch [3/5], Loss: 0.0656\n",
            "Epoch [4/5], Loss: 0.0603\n",
            "Epoch [5/5], Loss: 0.0569\n",
            "Test Accuracy of the model on the 10000 test images: 98.37%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B8sMakArVCWd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}