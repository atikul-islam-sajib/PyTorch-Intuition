{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lG4h90obOqXI"
      },
      "source": [
        "# Cloning the TransformerEncoderScratch Repository\n",
        "\n",
        "To get started with the Transformer Encoder, first, you need to clone the `TransformerEncoderScratch` repository from GitHub. This repository contains all the necessary code and examples to help you understand and implement the Transformer Encoder from scratch.\n",
        "\n",
        "## Step 1: Clone the Repository\n",
        "\n",
        "Open your terminal and run the following command:\n",
        "\n",
        "```bash\n",
        "!git clone https://github.com/atikul-islam-sajib/TransformerEncoderScratch.git\n",
        "```\n",
        "\n",
        "This command will create a directory named `TransformerEncoderScratch` in your current working directory, containing all the code and resources you need.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7m5uhFg5OqXN"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/atikul-islam-sajib/TransformerEncoderScratch.git"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Next Steps - Change the directory\n",
        "\n",
        "```bash\n",
        "cd TransformerEncoderScratch\n",
        "```"
      ],
      "metadata": {
        "id": "CRXNzFz9O0zX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cd TransformerEncoderScratch"
      ],
      "metadata": {
        "id": "7l0eyWsLO2ms"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "otzp5qP_OqXP"
      },
      "source": [
        "# Install the requirements\n",
        "\n",
        "To run the code, you need to install the required dependencies. Run the following command in your terminal:\n",
        "\n",
        "```bash\n",
        "pip install -r requirements.txt\n",
        "```\n",
        "\n",
        "This command will install all the necessary libraries and packages listed in the `requirements.txt` file."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "itOMvF89O5pe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXUhU0QKOqXR"
      },
      "source": [
        "## Inference the Transformer Encoder\n",
        "\n",
        "```python\n",
        "import torch\n",
        "from transformer import TransformerEncoder\n",
        "```\n",
        "\n",
        "- **torch**: PyTorch library used for tensor operations and neural network modules.\n",
        "- **TransformerEncoder**: Class imported from a local `transformer` module that implements the Transformer Encoder.\n",
        "\n",
        "### Script Attributes\n",
        "\n",
        "```python\n",
        "\"\"\"\n",
        "This script initializes a Transformer Encoder with specified parameters,\n",
        "creates a random embedding tensor, and prints the shapes of the embedding\n",
        "and the output tensors.\n",
        "\n",
        "Attributes:\n",
        "    batch_size (int): The batch size for the input tensor.\n",
        "    sequence_length (int): The sequence length for the input tensor.\n",
        "    model_dimension (int): The dimension of the model.\n",
        "    feed_forward (int): The dimension of the feed forward network.\n",
        "    number_heads (int): The number of attention heads.\n",
        "    dropout (float): The dropout rate.\n",
        "    epsilon (float): The epsilon value for numerical stability.\n",
        "\"\"\"\n",
        "```\n",
        "\n",
        "- **batch_size**: The number of samples processed together in one forward/backward pass. Here, it is set to 64.\n",
        "- **sequence_length**: The length of the input sequences. Here, it is set to 512.\n",
        "- **model_dimension**: The size of the input feature vector. Here, it is set to 768.\n",
        "- **feed_forward**: The dimension of the feed forward network inside the Transformer Encoder. Here, it is set to 2048.\n",
        "- **number_heads**: The number of attention heads in the multi-head attention mechanism. Here, it is set to 12.\n",
        "- **dropout**: The dropout rate used to prevent overfitting. Here, it is set to 0.1.\n",
        "- **epsilon**: A small value added to avoid division by zero during normalization. Here, it is set to 1e-6.\n",
        "\n",
        "### Creating the Embedding Tensor\n",
        "\n",
        "```python\n",
        "# Create a random embedding tensor with the specified shape\n",
        "embedding = torch.randn((batch_size, sequence_length, model_dimension))\n",
        "```\n",
        "\n",
        "- **embedding**: A random tensor of shape `(batch_size, sequence_length, model_dimension)` generated using `torch.randn`. This tensor simulates the input embeddings to the Transformer Encoder. In real, use *nn.Embedding()* to create the embedding layer.\n",
        "\n",
        "### Creating the Padding Mask Tensor\n",
        "\n",
        "```python\n",
        "# Create a random padding mask tensor\n",
        "padding_masked = torch.randn((batch_size, sequence_length))\n",
        "```\n",
        "\n",
        "- **padding_masked**: A random tensor of shape `(batch_size, sequence_length)` generated using `torch.randn`. This tensor simulates the padding mask that indicates which elements in the sequence are padding and should be ignored during attention calculations.\n",
        "\n",
        "### Initializing the Transformer Encoder\n",
        "\n",
        "```python\n",
        "# Initialize the Transformer Encoder with the specified parameters\n",
        "netTransformer = TransformerEncoder(\n",
        "    dimension=model_dimension,\n",
        "    heads=number_heads,\n",
        "    feed_forward=feed_forward,\n",
        "    dropout=dropout,\n",
        "    epsilon=epsilon,\n",
        "    mask=padding_masked,\n",
        ")\n",
        "```\n",
        "\n",
        "- **netTransformer**: An instance of the `TransformerEncoder` class, initialized with the specified parameters.\n",
        "\n",
        "### Printing the Shapes of Embedding and Output Tensors\n",
        "\n",
        "```python\n",
        "# Print the divider line\n",
        "print(\"|\", \"-\" * 100, \"|\")\n",
        "\n",
        "# Print the shape of the embedding tensor\n",
        "print(\"|\", \"\\tThe embedding shape is: \", embedding.size())\n",
        "\n",
        "# Pass the embedding through the Transformer Encoder and print the output shape\n",
        "print(\n",
        "    \"|\",\n",
        "    \"\\tThe output shape is: \",\n",
        "    netTransformer(embedding).size(),\n",
        ")  # (batch_size, sequence_length, model_dimension)\n",
        "\n",
        "# Print the closing divider line\n",
        "print(\"|\", \"-\" * 100, \"|\")\n",
        "```\n",
        "\n",
        "- The script prints a divider line for readability.\n",
        "- It prints the shape of the embedding tensor.\n",
        "- It passes the embedding tensor through the `netTransformer` (Transformer Encoder) and prints the shape of the resulting output tensor.\n",
        "- Finally, it prints a closing divider line."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fPKRkht8OqXS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformer import TransformerEncoder\n",
        "\n",
        "\"\"\"\n",
        "This script initializes a Transformer Encoder with specified parameters,\n",
        "creates a random embedding tensor, and prints the shapes of the embedding\n",
        "and the output tensors.\n",
        "\n",
        "Attributes:\n",
        "    batch_size (int): The batch size for the input tensor.\n",
        "    sequence_length (int): The sequence length for the input tensor.\n",
        "    model_dimension (int): The dimension of the model.\n",
        "    feed_forward (int): The dimension of the feed forward network.\n",
        "    number_heads (int): The number of attention heads.\n",
        "    dropout (float): The dropout rate.\n",
        "    epsilon (float): The epsilon value for numerical stability.\n",
        "\"\"\"\n",
        "\n",
        "batch_size = 64\n",
        "sequence_length = 512\n",
        "model_dimension = 768\n",
        "feed_forward = 2048\n",
        "number_heads = 12\n",
        "dropout = 0.1\n",
        "epsilon = 1e-6\n",
        "\n",
        "# Create a random embedding tensor with the specified shape\n",
        "embedding = torch.randn((batch_size, sequence_length, model_dimension))\n",
        "\n",
        "# Create a random padding mask tensor\n",
        "padding_masked = torch.randn((batch_size, sequence_length))\n",
        "\n",
        "# Initialize the Transformer Encoder with the specified parameters\n",
        "netTransformer = TransformerEncoder(\n",
        "    dimension=model_dimension,\n",
        "    heads=number_heads,\n",
        "    feed_forward=feed_forward,\n",
        "    dropout=dropout,\n",
        "    epsilon=epsilon,\n",
        "    mask=padding_masked,\n",
        ")\n",
        "\n",
        "# Print the divider line\n",
        "print(\"|\", \"-\" * 100, \"|\")\n",
        "\n",
        "# Print the shape of the embedding tensor\n",
        "print(\"|\", \"\\tThe embedding shape is: \", embedding.size())\n",
        "\n",
        "# Pass the embedding through the Transformer Encoder and print the output shape\n",
        "print(\n",
        "    \"|\",\n",
        "    \"\\tThe output shape is: \",\n",
        "    netTransformer(embedding).size(),\n",
        ")  # (batch_size, sequence_length, model_dimension)\n",
        "\n",
        "# Print the closing divider line\n",
        "print(\"|\", \"-\" * 100, \"|\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "GPSG",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}