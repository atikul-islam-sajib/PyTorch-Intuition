{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "id": "oi30nNnHJky2",
        "outputId": "f1aed2a3-2484-4ce6-e110-54920b12a32b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'hidden_size' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7f510370d59c>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0mtrain_and_test\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-7f510370d59c>\u001b[0m in \u001b[0;36mtrain_and_test\u001b[0;34m()\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-7f510370d59c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text, target_img)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;31m# Combine text and image features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m             \u001b[0mcombined\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_features\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtext_features\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-7f510370d59c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# [batch_size, hidden_size, num_patches]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Reshape to 4x4 feature map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupsample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Upsample to 64x64\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'hidden_size' is not defined"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the Vision Transformer (ViT) component\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, image_size=64, patch_size=16, hidden_size=256, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = nn.Conv2d(3, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional embedding\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, hidden_size))\n",
        "\n",
        "        # Transformer encoder\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size*4\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, 3, image_size, image_size]\n",
        "        x = self.patch_embed(x)  # [batch_size, hidden_size, num_patches_h, num_patches_w]\n",
        "        x = x.flatten(2)  # [batch_size, hidden_size, num_patches]\n",
        "        x = x.transpose(1, 2)  # [batch_size, num_patches, hidden_size]\n",
        "        x = x + self.pos_embed\n",
        "        x = self.transformer(x)\n",
        "        return x\n",
        "\n",
        "# 2. Define the Text Encoder using Transformer\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size=1000, hidden_size=256, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size*4\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_length]\n",
        "        x = self.embedding(x)\n",
        "        x = self.transformer(x)\n",
        "        return x.mean(dim=1)  # Global average pooling\n",
        "\n",
        "# 3. Define the Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size=256):\n",
        "        super().__init__()\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_size, 128, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, num_patches, hidden_size]\n",
        "        batch_size = x.size(0)\n",
        "        x = x.transpose(1, 2)  # [batch_size, hidden_size, num_patches]\n",
        "        x = x.view(batch_size, hidden_size, 4, 4)  # Reshape to 4x4 feature map\n",
        "        x = self.upsample(x)  # Upsample to 64x64\n",
        "        return x\n",
        "\n",
        "# 4. Combine into Text2Image model\n",
        "class Text2Image(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden_size = 256\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.vit_encoder = ViTEncoder()\n",
        "        self.decoder = Decoder()\n",
        "\n",
        "    def forward(self, text, target_img=None):\n",
        "        # Encode text\n",
        "        text_features = self.text_encoder(text)  # [batch_size, hidden_size]\n",
        "\n",
        "        # If training, use target image\n",
        "        if target_img is not None:\n",
        "            img_features = self.vit_encoder(target_img)\n",
        "            # Combine text and image features\n",
        "            combined = img_features + text_features.unsqueeze(1)\n",
        "            output = self.decoder(combined)\n",
        "            return output\n",
        "\n",
        "        # If inference, generate from text only\n",
        "        batch_size = text_features.size(0)\n",
        "        dummy_features = torch.zeros(batch_size, 16, self.hidden_size).to(text_features.device)\n",
        "        combined = dummy_features + text_features.unsqueeze(1)\n",
        "        return self.decoder(combined)\n",
        "\n",
        "# 5. Create dummy dataset\n",
        "class DummyTextImageDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000):\n",
        "        self.num_samples = num_samples\n",
        "        self.vocab_size = 1000\n",
        "        self.seq_length = 10\n",
        "        self.image_size = 64\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Dummy text (random token IDs)\n",
        "        text = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
        "        # Dummy image (random pixels)\n",
        "        image = torch.rand(3, self.image_size, self.image_size)\n",
        "        return text, image\n",
        "\n",
        "# 6. Training and Testing\n",
        "def train_and_test():\n",
        "    # Device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    # Model\n",
        "    model = Text2Image().to(device)\n",
        "\n",
        "    # Dataset and DataLoader\n",
        "    dataset = DummyTextImageDataset()\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    # Optimizer and loss\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Training\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (text, image) in enumerate(train_loader):\n",
        "            text, image = text.to(device), image.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(text, image)\n",
        "            loss = criterion(output, image)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    # Testing\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text, image in test_loader:\n",
        "            text, image = text.to(device), image.to(device)\n",
        "            output = model(text, image)\n",
        "            loss = criterion(output, image)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_test()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import models\n",
        "import numpy as np\n",
        "\n",
        "# 1. Define the Vision Transformer (ViT) component\n",
        "class ViTEncoder(nn.Module):\n",
        "    def __init__(self, image_size=64, patch_size=16, hidden_size=256, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.num_patches = (image_size // patch_size) ** 2\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # Patch embedding\n",
        "        self.patch_embed = nn.Conv2d(3, hidden_size, kernel_size=patch_size, stride=patch_size)\n",
        "\n",
        "        # Positional embedding\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, self.num_patches, hidden_size))\n",
        "\n",
        "        # Transformer encoder with batch_first=True\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size*4,\n",
        "            batch_first=True  # Added this to fix the warning\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.patch_embed(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(1, 2)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.transformer(x)\n",
        "        return x\n",
        "\n",
        "# 2. Define the Text Encoder\n",
        "class TextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size=1000, hidden_size=256, num_heads=8):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(\n",
        "            d_model=hidden_size,\n",
        "            nhead=num_heads,\n",
        "            dim_feedforward=hidden_size*4,\n",
        "            batch_first=True  # Added this to fix the warning\n",
        "        )\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.embedding(x)\n",
        "        x = self.transformer(x)\n",
        "        return x.mean(dim=1)\n",
        "\n",
        "# 3. Define the Decoder\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, hidden_size=256):  # Added hidden_size as parameter\n",
        "        super().__init__()\n",
        "        self.hidden_size = hidden_size  # Store hidden_size\n",
        "        self.upsample = nn.Sequential(\n",
        "            nn.ConvTranspose2d(hidden_size, 128, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, 2, 1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, 2, 1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        batch_size = x.size(0)\n",
        "        x = x.transpose(1, 2)  # [batch_size, hidden_size, num_patches]\n",
        "        x = x.view(batch_size, self.hidden_size, 4, 4)  # Use self.hidden_size instead of hidden_size\n",
        "        x = self.upsample(x)\n",
        "        return x\n",
        "\n",
        "# 4. Combine into Text2Image model\n",
        "class Text2Image(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.hidden_size = 256\n",
        "        self.text_encoder = TextEncoder()\n",
        "        self.vit_encoder = ViTEncoder()\n",
        "        self.decoder = Decoder(hidden_size=self.hidden_size)  # Pass hidden_size\n",
        "\n",
        "    def forward(self, text, target_img=None):\n",
        "        text_features = self.text_encoder(text)\n",
        "\n",
        "        if target_img is not None:\n",
        "            img_features = self.vit_encoder(target_img)\n",
        "            combined = img_features + text_features.unsqueeze(1)\n",
        "            output = self.decoder(combined)\n",
        "            return output\n",
        "\n",
        "        batch_size = text_features.size(0)\n",
        "        dummy_features = torch.zeros(batch_size, 16, self.hidden_size).to(text_features.device)\n",
        "        combined = dummy_features + text_features.unsqueeze(1)\n",
        "        return self.decoder(combined)\n",
        "\n",
        "# 5. Create dummy dataset\n",
        "class DummyTextImageDataset(Dataset):\n",
        "    def __init__(self, num_samples=1000):\n",
        "        self.num_samples = num_samples\n",
        "        self.vocab_size = 1000\n",
        "        self.seq_length = 10\n",
        "        self.image_size = 64\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = torch.randint(0, self.vocab_size, (self.seq_length,))\n",
        "        image = torch.rand(3, self.image_size, self.image_size)\n",
        "        return text, image\n",
        "\n",
        "# 6. Training and Testing\n",
        "def train_and_test():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = Text2Image().to(device)\n",
        "\n",
        "    dataset = DummyTextImageDataset()\n",
        "    train_size = int(0.8 * len(dataset))\n",
        "    test_size = len(dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(dataset, [train_size, test_size])\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    num_epochs = 5\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        train_loss = 0\n",
        "        for batch_idx, (text, image) in enumerate(train_loader):\n",
        "            text, image = text.to(device), image.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(text, image)\n",
        "            loss = criterion(output, image)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss/len(train_loader):.4f}\")\n",
        "\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for text, image in test_loader:\n",
        "            text, image = text.to(device), image.to(device)\n",
        "            output = model(text, image)\n",
        "            loss = criterion(output, image)\n",
        "            test_loss += loss.item()\n",
        "\n",
        "    print(f\"Test Loss: {test_loss/len(test_loader):.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_and_test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hkjOUzTQJrqI",
        "outputId": "91fe1855-05c0-4fa8-f3c4-a7982915e69e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5, Train Loss: 0.0834\n",
            "Epoch 2/5, Train Loss: 0.0833\n",
            "Epoch 3/5, Train Loss: 0.0834\n",
            "Epoch 4/5, Train Loss: 0.0833\n",
            "Epoch 5/5, Train Loss: 0.0833\n",
            "Test Loss: 0.0833\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/image_dataset.zip -d /content/image_dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELlLe7gqKbQP",
        "outputId": "6ef74c09-d98e-4e9c-989a-d0641ed8206a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/image_dataset.zip\n",
            " extracting: /content/image_dataset/CXR1849_IM-0550-1001.png  \n",
            " extracting: /content/image_dataset/CXR42_IM-2063-1001.png  \n",
            " extracting: /content/image_dataset/CXR1493_IM-0318-1001.png  \n",
            " extracting: /content/image_dataset/CXR2368_IM-0928-1001.png  \n",
            " extracting: /content/image_dataset/CXR279_IM-1224-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR3576_IM-1757-1001.png  \n",
            " extracting: /content/image_dataset/CXR1248_IM-0168-1001.png  \n",
            " extracting: /content/image_dataset/CXR2523_IM-1041-1001.png  \n",
            " extracting: /content/image_dataset/CXR1388_IM-0246-1001.png  \n",
            " extracting: /content/image_dataset/CXR841_IM-2365-1001.png  \n",
            " extracting: /content/image_dataset/CXR2827_IM-1246-1001.png  \n",
            " extracting: /content/image_dataset/CXR2546_IM-1055-1001.png  \n",
            " extracting: /content/image_dataset/CXR520_IM-2131-1001.png  \n",
            " extracting: /content/image_dataset/CXR2373_IM-0934-1001.png  \n",
            " extracting: /content/image_dataset/CXR3701_IM-1848-1001.png  \n",
            " extracting: /content/image_dataset/CXR1639_IM-0418-1001.png  \n",
            " extracting: /content/image_dataset/CXR2616_IM-1106-1001.png  \n",
            " extracting: /content/image_dataset/CXR3895_IM-1977-1001.png  \n",
            " extracting: /content/image_dataset/CXR3642_IM-1806-1001.png  \n",
            " extracting: /content/image_dataset/CXR254_IM-1051-1001.png  \n",
            " extracting: /content/image_dataset/CXR972_IM-2461-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR3620_IM-1791-1001-0002.png  \n",
            " extracting: /content/image_dataset/CXR788_IM-2328-1001.png  \n",
            " extracting: /content/image_dataset/CXR838_IM-2362-1001.png  \n",
            " extracting: /content/image_dataset/CXR1342_IM-0221-1001.png  \n",
            " extracting: /content/image_dataset/CXR2614_IM-1105-1001.png  \n",
            " extracting: /content/image_dataset/CXR832_IM-2359-1001.png  \n",
            " extracting: /content/image_dataset/CXR3141_IM-1477-1001.png  \n",
            " extracting: /content/image_dataset/CXR2287_IM-0872-1001.png  \n",
            " extracting: /content/image_dataset/CXR3562_IM-1747-1001.png  \n",
            " extracting: /content/image_dataset/CXR1230_IM-0154-1001.png  \n",
            " extracting: /content/image_dataset/CXR478_IM-2101-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR1717_IM-0473-1001.png  \n",
            " extracting: /content/image_dataset/CXR90_IM-2407-1001.png  \n",
            " extracting: /content/image_dataset/CXR1927_IM-0600-1001.png  \n",
            " extracting: /content/image_dataset/CXR2404_IM-0952-1001.png  \n",
            " extracting: /content/image_dataset/CXR2611_IM-1102-1001.png  \n",
            " extracting: /content/image_dataset/CXR493_IM-2113-1001.png  \n",
            " extracting: /content/image_dataset/CXR2341_IM-0907-1001.png  \n",
            " extracting: /content/image_dataset/CXR3863_IM-1957-1001.png  \n",
            " extracting: /content/image_dataset/CXR2079_IM-0711-1001.png  \n",
            " extracting: /content/image_dataset/CXR521_IM-2132-1001.png  \n",
            " extracting: /content/image_dataset/CXR3491_IM-1698-1001.png  \n",
            " extracting: /content/image_dataset/CXR1914_IM-0595-1001.png  \n",
            " extracting: /content/image_dataset/CXR2436_IM-0976-1001.png  \n",
            " extracting: /content/image_dataset/CXR3845_IM-1945-1001.png  \n",
            " extracting: /content/image_dataset/CXR2570_IM-1073-1001.png  \n",
            " extracting: /content/image_dataset/CXR2669_IM-1147-1001.png  \n",
            " extracting: /content/image_dataset/CXR2889_IM-1291-1001.png  \n",
            " extracting: /content/image_dataset/CXR3654_IM-1816-1001.png  \n",
            " extracting: /content/image_dataset/CXR2539_IM-1050-1001.png  \n",
            " extracting: /content/image_dataset/CXR6_IM-2192-1001.png  \n",
            " extracting: /content/image_dataset/CXR956_IM-2449-1001.png  \n",
            " extracting: /content/image_dataset/CXR2862_IM-1270-1001.png  \n",
            " extracting: /content/image_dataset/CXR2835_IM-1251-1001.png  \n",
            " extracting: /content/image_dataset/CXR3262_IM-1548-1001.png  \n",
            " extracting: /content/image_dataset/CXR3245_IM-1537-1001.png  \n",
            " extracting: /content/image_dataset/CXR2564_IM-1067-1001.png  \n",
            " extracting: /content/image_dataset/CXR3242_IM-1534-1001.png  \n",
            " extracting: /content/image_dataset/CXR2918_IM-1320-1001.png  \n",
            " extracting: /content/image_dataset/CXR3521_IM-1719-1001.png  \n",
            " extracting: /content/image_dataset/CXR3789_IM-1902-1001.png  \n",
            " extracting: /content/image_dataset/CXR3988_IM-2041-1001.png  \n",
            " extracting: /content/image_dataset/CXR1516_IM-0334-1001.png  \n",
            " extracting: /content/image_dataset/CXR2435_IM-0976-1001.png  \n",
            " extracting: /content/image_dataset/CXR3003_IM-1388-1001.png  \n",
            " extracting: /content/image_dataset/CXR157_IM-0372-1001.png  \n",
            " extracting: /content/image_dataset/CXR3128_IM-1471-1001.png  \n",
            " extracting: /content/image_dataset/CXR3857_IM-1952-1001.png  \n",
            " extracting: /content/image_dataset/CXR3935_IM-2006-1001.png  \n",
            " extracting: /content/image_dataset/CXR1782_IM-0510-1001.png  \n",
            " extracting: /content/image_dataset/CXR2705_IM-1171-1001.png  \n",
            " extracting: /content/image_dataset/CXR1874_IM-0565-1001.png  \n",
            " extracting: /content/image_dataset/CXR3919_IM-1992-1001.png  \n",
            " extracting: /content/image_dataset/CXR1700_IM-0462-1001.png  \n",
            " extracting: /content/image_dataset/CXR2955_IM-1352-1001.png  \n",
            " extracting: /content/image_dataset/CXR2794_IM-1226-1001.png  \n",
            " extracting: /content/image_dataset/CXR360_IM-1776-1001.png  \n",
            " extracting: /content/image_dataset/CXR2779_IM-1218-1001.png  \n",
            " extracting: /content/image_dataset/CXR1032_IM-0026-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR3942_IM-2013-1001.png  \n",
            " extracting: /content/image_dataset/CXR84_IM-2363-1001.png  \n",
            " extracting: /content/image_dataset/CXR3677_IM-1830-1001.png  \n",
            " extracting: /content/image_dataset/CXR1464_IM-0301-1001.png  \n",
            " extracting: /content/image_dataset/CXR556_IM-2156-1001-0002.png  \n",
            " extracting: /content/image_dataset/CXR1539_IM-0349-1001.png  \n",
            " extracting: /content/image_dataset/CXR2880_IM-1284-1001.png  \n",
            " extracting: /content/image_dataset/CXR1704_IM-0464-1001.png  \n",
            " extracting: /content/image_dataset/CXR984_IM-2471-1001.png  \n",
            " extracting: /content/image_dataset/CXR1598_IM-0389-1001.png  \n",
            " extracting: /content/image_dataset/CXR1193_IM-0129-1001.png  \n",
            " extracting: /content/image_dataset/CXR1256_IM-0173-1001.png  \n",
            " extracting: /content/image_dataset/CXR3987_IM-2041-1001.png  \n",
            " extracting: /content/image_dataset/CXR685_IM-2254-1001.png  \n",
            " extracting: /content/image_dataset/CXR3028_IM-1403-1001.png  \n",
            " extracting: /content/image_dataset/CXR2843_IM-1254-1001-0002.png  \n",
            " extracting: /content/image_dataset/CXR2363_IM-0926-1001.png  \n",
            " extracting: /content/image_dataset/CXR1589_IM-0382-1001.png  \n",
            " extracting: /content/image_dataset/CXR694_IM-2260-1001.png  \n",
            " extracting: /content/image_dataset/CXR2342_IM-0907-1001.png  \n",
            " extracting: /content/image_dataset/CXR3114_IM-1462-1001.png  \n",
            " extracting: /content/image_dataset/CXR2416_IM-0961-1001.png  \n",
            " extracting: /content/image_dataset/CXR1035_IM-0028-1001.png  \n",
            " extracting: /content/image_dataset/CXR2094_IM-0724-1001.png  \n",
            " extracting: /content/image_dataset/CXR125_IM-0169-1001.png  \n",
            " extracting: /content/image_dataset/CXR935_IM-2432-1001.png  \n",
            " extracting: /content/image_dataset/CXR2030_IM-0675-1001.png  \n",
            " extracting: /content/image_dataset/CXR3227_IM-1525-1001.png  \n",
            " extracting: /content/image_dataset/CXR1369_IM-0238-1001.png  \n",
            " extracting: /content/image_dataset/CXR252_IM-1038-1001.png  \n",
            " extracting: /content/image_dataset/CXR2210_IM-0817-1001.png  \n",
            " extracting: /content/image_dataset/CXR1105_IM-0072-1001-0002.png  \n",
            " extracting: /content/image_dataset/CXR2443_IM-0980-1001.png  \n",
            " extracting: /content/image_dataset/CXR3077_IM-1438-1001.png  \n",
            " extracting: /content/image_dataset/CXR3878_IM-1968-1001.png  \n",
            " extracting: /content/image_dataset/CXR615_IM-2200-1001.png  \n",
            " extracting: /content/image_dataset/CXR2569_IM-1071-1001.png  \n",
            " extracting: /content/image_dataset/CXR1057_IM-0041-1001.png  \n",
            " extracting: /content/image_dataset/CXR937_IM-2433-1001.png  \n",
            " extracting: /content/image_dataset/CXR1773_IM-0506-1001.png  \n",
            " extracting: /content/image_dataset/CXR1276_IM-0184-1001.png  \n",
            " extracting: /content/image_dataset/CXR3841_IM-1942-1001.png  \n",
            " extracting: /content/image_dataset/CXR2477_IM-1006-1001.png  \n",
            " extracting: /content/image_dataset/CXR1735_IM-0484-1001.png  \n",
            " extracting: /content/image_dataset/CXR1301_IM-0198-1001.png  \n",
            " extracting: /content/image_dataset/CXR61_IM-2197-1001.png  \n",
            " extracting: /content/image_dataset/CXR2456_IM-0989-1001.png  \n",
            " extracting: /content/image_dataset/CXR3575_IM-1757-1001.png  \n",
            " extracting: /content/image_dataset/CXR1402_IM-0257-1001.png  \n",
            " extracting: /content/image_dataset/CXR358_IM-1759-1001.png  \n",
            " extracting: /content/image_dataset/CXR2202_IM-0811-1001.png  \n",
            " extracting: /content/image_dataset/CXR3379_IM-1627-1001.png  \n",
            " extracting: /content/image_dataset/CXR1588_IM-0382-1001.png  \n",
            " extracting: /content/image_dataset/CXR3005_IM-1388-1001.png  \n",
            " extracting: /content/image_dataset/CXR3432_IM-1661-1001.png  \n",
            " extracting: /content/image_dataset/CXR3088_IM-1444-1001.png  \n",
            " extracting: /content/image_dataset/CXR759_IM-2309-1001.png  \n",
            " extracting: /content/image_dataset/CXR1797_IM-0517-1001.png  \n",
            " extracting: /content/image_dataset/CXR2530_IM-1045-1001.png  \n",
            " extracting: /content/image_dataset/CXR549_IM-2153-1001.png  \n",
            " extracting: /content/image_dataset/CXR3312_IM-1586-1001.png  \n",
            " extracting: /content/image_dataset/CXR2218_IM-0823-1001.png  \n",
            " extracting: /content/image_dataset/CXR2626_IM-1113-1001.png  \n",
            " extracting: /content/image_dataset/CXR1679_IM-0448-1001.png  \n",
            " extracting: /content/image_dataset/CXR1204_IM-0138-1001.png  \n",
            " extracting: /content/image_dataset/CXR1465_IM-0302-1001.png  \n",
            " extracting: /content/image_dataset/CXR3310_IM-1585-1001.png  \n",
            " extracting: /content/image_dataset/CXR4_IM-2050-1001.png  \n",
            " extracting: /content/image_dataset/CXR1170_IM-0115-1001.png  \n",
            " extracting: /content/image_dataset/CXR3126_IM-1470-1001.png  \n",
            " extracting: /content/image_dataset/CXR3657_IM-1818-1001.png  \n",
            " extracting: /content/image_dataset/CXR2805_IM-1235-1001.png  \n",
            " extracting: /content/image_dataset/CXR1663_IM-0439-1001.png  \n",
            " extracting: /content/image_dataset/CXR2033_IM-0678-1001.png  \n",
            " extracting: /content/image_dataset/CXR3020_IM-1395-1001.png  \n",
            " extracting: /content/image_dataset/CXR858_IM-2379-1001.png  \n",
            " extracting: /content/image_dataset/CXR608_IM-2196-1001.png  \n",
            " extracting: /content/image_dataset/CXR3095_IM-1448-1001.png  \n",
            " extracting: /content/image_dataset/CXR1751_IM-0494-1001.png  \n",
            " extracting: /content/image_dataset/CXR338_IM-1628-1001.png  \n",
            " extracting: /content/image_dataset/CXR3567_IM-1752-1001.png  \n",
            " extracting: /content/image_dataset/CXR839_IM-2363-1001.png  \n",
            " extracting: /content/image_dataset/CXR429_IM-2070-1001.png  \n",
            " extracting: /content/image_dataset/CXR3697_IM-1846-1001.png  \n",
            " extracting: /content/image_dataset/CXR620_IM-2202-1001.png  \n",
            " extracting: /content/image_dataset/CXR1062_IM-0043-1001.png  \n",
            " extracting: /content/image_dataset/CXR3911_IM-1987-1001.png  \n",
            " extracting: /content/image_dataset/CXR3506_IM-1708-1001.png  \n",
            " extracting: /content/image_dataset/CXR2950_IM-1348-1001.png  \n",
            " extracting: /content/image_dataset/CXR1734_IM-0484-1001.png  \n",
            " extracting: /content/image_dataset/CXR1361_IM-0235-1001.png  \n",
            " extracting: /content/image_dataset/CXR768_IM-2313-1001.png  \n",
            " extracting: /content/image_dataset/CXR3561_IM-1746-1001.png  \n",
            " extracting: /content/image_dataset/CXR748_IM-2302-1001.png  \n",
            " extracting: /content/image_dataset/CXR92_IM-2422-1001.png  \n",
            " extracting: /content/image_dataset/CXR1372_IM-0239-1001.png  \n",
            " extracting: /content/image_dataset/CXR2351_IM-0917-1001.png  \n",
            " extracting: /content/image_dataset/CXR859_IM-2380-1001.png  \n",
            " extracting: /content/image_dataset/CXR481_IM-2105-1001.png  \n",
            " extracting: /content/image_dataset/CXR1315_IM-0204-1001.png  \n",
            " extracting: /content/image_dataset/CXR2605_IM-1095-1001.png  \n",
            " extracting: /content/image_dataset/CXR2213_IM-0819-1001.png  \n",
            " extracting: /content/image_dataset/CXR499_IM-2116-1001.png  \n",
            " extracting: /content/image_dataset/CXR227_IM-0859-1001.png  \n",
            " extracting: /content/image_dataset/CXR3930_IM-2003-1001.png  \n",
            " extracting: /content/image_dataset/CXR3867_IM-1960-1001.png  \n",
            " extracting: /content/image_dataset/CXR2119_IM-0746-1001.png  \n",
            " extracting: /content/image_dataset/CXR1786_IM-0512-1001.png  \n",
            " extracting: /content/image_dataset/CXR31_IM-1450-1001.png  \n",
            " extracting: /content/image_dataset/CXR1091_IM-0062-1001.png  \n",
            " extracting: /content/image_dataset/CXR1002_IM-0004-1001.png  \n",
            " extracting: /content/image_dataset/CXR2321_IM-0894-1001.png  \n",
            " extracting: /content/image_dataset/CXR300_IM-1385-1001.png  \n",
            " extracting: /content/image_dataset/CXR2698_IM-1167-1001.png  \n",
            " extracting: /content/image_dataset/CXR781_IM-2323-1001.png  \n",
            " extracting: /content/image_dataset/CXR3294_IM-1573-1001.png  \n",
            " extracting: /content/image_dataset/CXR1743_IM-0489-1001.png  \n",
            " extracting: /content/image_dataset/CXR3118_IM-1466-1001.png  \n",
            " extracting: /content/image_dataset/CXR3192_IM-1505-1001.png  \n",
            " extracting: /content/image_dataset/CXR3102_IM-1454-1001.png  \n",
            " extracting: /content/image_dataset/CXR2599_IM-1089-1001.png  \n",
            " extracting: /content/image_dataset/CXR538_IM-2144-1001.png  \n",
            " extracting: /content/image_dataset/CXR3827_IM-1932-1001.png  \n",
            " extracting: /content/image_dataset/CXR726_IM-2286-1001.png  \n",
            " extracting: /content/image_dataset/CXR3668_IM-1825-1001.png  \n",
            " extracting: /content/image_dataset/CXR2551_IM-1058-1001.png  \n",
            " extracting: /content/image_dataset/CXR3597_IM-1775-1001.png  \n",
            " extracting: /content/image_dataset/CXR370_IM-1848-1001.png  \n",
            " extracting: /content/image_dataset/CXR1997_IM-0651-1001.png  \n",
            " extracting: /content/image_dataset/CXR3270_IM-1552-1001.png  \n",
            " extracting: /content/image_dataset/CXR3487_IM-1696-1001.png  \n",
            " extracting: /content/image_dataset/CXR2005_IM-0656-1001.png  \n",
            " extracting: /content/image_dataset/CXR3216_IM-1520-1001.png  \n",
            " extracting: /content/image_dataset/CXR3226_IM-1525-1001.png  \n",
            " extracting: /content/image_dataset/CXR3362_IM-1615-1001.png  \n",
            " extracting: /content/image_dataset/CXR3378_IM-1627-1001.png  \n",
            " extracting: /content/image_dataset/CXR3168_IM-1492-1001.png  \n",
            " extracting: /content/image_dataset/CXR708_IM-2271-1001.png  \n",
            " extracting: /content/image_dataset/CXR856_IM-2377-1001.png  \n",
            " extracting: /content/image_dataset/CXR861_IM-2382-1001.png  \n",
            " extracting: /content/image_dataset/CXR3727_IM-1863-1001.png  \n",
            " extracting: /content/image_dataset/CXR2738_IM-1192-1001.png  \n",
            " extracting: /content/image_dataset/CXR3534_IM-1727-1001.png  \n",
            " extracting: /content/image_dataset/CXR1661_IM-0437-1001.png  \n",
            " extracting: /content/image_dataset/CXR3714_IM-1856-1001.png  \n",
            " extracting: /content/image_dataset/CXR529_IM-2137-1001.png  \n",
            " extracting: /content/image_dataset/CXR2926_IM-1328-1001.png  \n",
            " extracting: /content/image_dataset/CXR3596_IM-1774-1001.png  \n",
            " extracting: /content/image_dataset/CXR1417_IM-0266-1001.png  \n",
            " extracting: /content/image_dataset/CXR2301_IM-0881-1001.png  \n",
            " extracting: /content/image_dataset/CXR1133_IM-0090-1001.png  \n",
            " extracting: /content/image_dataset/CXR696_IM-2261-1001-0002.png  \n",
            " extracting: /content/image_dataset/CXR686_IM-2254-1001.png  \n",
            " extracting: /content/image_dataset/CXR2355_IM-0919-1001.png  \n",
            " extracting: /content/image_dataset/CXR2579_IM-1078-1001.png  \n",
            " extracting: /content/image_dataset/CXR260_IM-1090-1001.png  \n",
            " extracting: /content/image_dataset/CXR1777_IM-0509-1001.png  \n",
            " extracting: /content/image_dataset/CXR1352_IM-0229-1001.png  \n",
            " extracting: /content/image_dataset/CXR2942_IM-1343-1001.png  \n",
            " extracting: /content/image_dataset/CXR3836_IM-1939-1001.png  \n",
            " extracting: /content/image_dataset/CXR3004_IM-1388-1001.png  \n",
            " extracting: /content/image_dataset/CXR2490_IM-1017-1001.png  \n",
            " extracting: /content/image_dataset/CXR3105_IM-1456-1001.png  \n",
            " extracting: /content/image_dataset/CXR95_IM-2445-1001.png  \n",
            " extracting: /content/image_dataset/CXR2522_IM-1040-1001.png  \n",
            " extracting: /content/image_dataset/CXR1447_IM-0289-1001.png  \n",
            " extracting: /content/image_dataset/CXR2739_IM-1193-1001.png  \n",
            " extracting: /content/image_dataset/CXR3384_IM-1631-1001.png  \n",
            " extracting: /content/image_dataset/CXR891_IM-2403-1001.png  \n",
            " extracting: /content/image_dataset/CXR2544_IM-1054-1001.png  \n",
            " extracting: /content/image_dataset/CXR568_IM-2168-1001.png  \n",
            " extracting: /content/image_dataset/CXR2891_IM-1294-1001.png  \n",
            " extracting: /content/image_dataset/CXR2854_IM-1262-1001.png  \n",
            " extracting: /content/image_dataset/CXR1969_IM-0630-1001.png  \n",
            " extracting: /content/image_dataset/CXR480_IM-2104-1001.png  \n",
            " extracting: /content/image_dataset/CXR1870_IM-0563-1001.png  \n",
            " extracting: /content/image_dataset/CXR3338_IM-1599-1001.png  \n",
            " extracting: /content/image_dataset/CXR402_IM-2051-1001.png  \n",
            " extracting: /content/image_dataset/CXR347_IM-1686-1001.png  \n",
            " extracting: /content/image_dataset/CXR2632_IM-1119-1001.png  \n",
            " extracting: /content/image_dataset/CXR634_IM-2214-1001.png  \n",
            " extracting: /content/image_dataset/CXR1066_IM-0047-1001.png  \n",
            " extracting: /content/image_dataset/CXR2077_IM-0710-1001.png  \n",
            " extracting: /content/image_dataset/CXR457_IM-2088-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR2809_IM-1238-1001.png  \n",
            " extracting: /content/image_dataset/CXR1565_IM-0368-1001.png  \n",
            " extracting: /content/image_dataset/CXR1095_IM-0066-1001.png  \n",
            " extracting: /content/image_dataset/CXR995_IM-2478-1001.png  \n",
            " extracting: /content/image_dataset/CXR600_IM-2192-1001.png  \n",
            " extracting: /content/image_dataset/CXR129_IM-0189-1001.png  \n",
            " extracting: /content/image_dataset/CXR1614_IM-0397-1001.png  \n",
            " extracting: /content/image_dataset/CXR1670_IM-0441-1001.png  \n",
            " extracting: /content/image_dataset/CXR2728_IM-1187-1001.png  \n",
            " extracting: /content/image_dataset/CXR2935_IM-1337-1001.png  \n",
            " extracting: /content/image_dataset/CXR2067_IM-0701-1001.png  \n",
            " extracting: /content/image_dataset/CXR1508_IM-0330-1001.png  \n",
            " extracting: /content/image_dataset/CXR3324_IM-1590-1001.png  \n",
            " extracting: /content/image_dataset/CXR2231_IM-0831-1001.png  \n",
            " extracting: /content/image_dataset/CXR2857_IM-1264-1001.png  \n",
            " extracting: /content/image_dataset/CXR2048_IM-0688-1001.png  \n",
            " extracting: /content/image_dataset/CXR306_IM-1426-1001.png  \n",
            " extracting: /content/image_dataset/CXR847_IM-2369-1001.png  \n",
            " extracting: /content/image_dataset/CXR3109_IM-1458-1001.png  \n",
            " extracting: /content/image_dataset/CXR1228_IM-0151-1001.png  \n",
            " extracting: /content/image_dataset/CXR1730_IM-0481-1001.png  \n",
            " extracting: /content/image_dataset/CXR69_IM-2258-1001.png  \n",
            " extracting: /content/image_dataset/CXR785_IM-2325-1001.png  \n",
            " extracting: /content/image_dataset/CXR2292_IM-0874-1001.png  \n",
            " extracting: /content/image_dataset/CXR3693_IM-1844-1001.png  \n",
            " extracting: /content/image_dataset/CXR1365_IM-0237-1001.png  \n",
            " extracting: /content/image_dataset/CXR2763_IM-1209-1001.png  \n",
            " extracting: /content/image_dataset/CXR3193_IM-1505-1001.png  \n",
            " extracting: /content/image_dataset/CXR2550_IM-1058-1001.png  \n",
            " extracting: /content/image_dataset/CXR78_IM-2322-1001.png  \n",
            " extracting: /content/image_dataset/CXR1414_IM-0264-1001.png  \n",
            " extracting: /content/image_dataset/CXR1640_IM-0420-1001.png  \n",
            " extracting: /content/image_dataset/CXR2729_IM-1187-1001.png  \n",
            " extracting: /content/image_dataset/CXR2826_IM-1246-1001.png  \n",
            " extracting: /content/image_dataset/CXR3999_IM-2049-1001.png  \n",
            " extracting: /content/image_dataset/CXR2211_IM-0818-1001.png  \n",
            " extracting: /content/image_dataset/CXR2495_IM-1021-1001.png  \n",
            " extracting: /content/image_dataset/CXR695_IM-2261-1001.png  \n",
            " extracting: /content/image_dataset/CXR616_IM-2200-1001.png  \n",
            " extracting: /content/image_dataset/CXR2892_IM-1295-1001.png  \n",
            " extracting: /content/image_dataset/CXR1188_IM-0127-1001.png  \n",
            " extracting: /content/image_dataset/CXR3821_IM-1929-1001.png  \n",
            " extracting: /content/image_dataset/CXR949_IM-2444-1001.png  \n",
            " extracting: /content/image_dataset/CXR2084_IM-0715-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR455_IM-2086-1001.png  \n",
            " extracting: /content/image_dataset/CXR3731_IM-1865-1001.png  \n",
            " extracting: /content/image_dataset/CXR108_IM-0056-1001.png  \n",
            " extracting: /content/image_dataset/CXR3735_IM-1866-1001.png  \n",
            " extracting: /content/image_dataset/CXR323_IM-1526-1001.png  \n",
            " extracting: /content/image_dataset/CXR1058_IM-0041-1001.png  \n",
            " extracting: /content/image_dataset/CXR2671_IM-1148-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR3174_IM-1496-1001.png  \n",
            " extracting: /content/image_dataset/CXR720_IM-2281-1001.png  \n",
            " extracting: /content/image_dataset/CXR2354_IM-0918-1001.png  \n",
            " extracting: /content/image_dataset/CXR3334_IM-1598-1001.png  \n",
            " extracting: /content/image_dataset/CXR911_IM-2417-1001.png  \n",
            " extracting: /content/image_dataset/CXR2536_IM-1049-1001.png  \n",
            " extracting: /content/image_dataset/CXR3823_IM-1930-1001.png  \n",
            " extracting: /content/image_dataset/CXR2588_IM-1083-1001.png  \n",
            " extracting: /content/image_dataset/CXR3263_IM-1549-1001.png  \n",
            " extracting: /content/image_dataset/CXR461_IM-2090-1001.png  \n",
            " extracting: /content/image_dataset/CXR1863_IM-0558-1001.png  \n",
            " extracting: /content/image_dataset/CXR2460_IM-0993-1001.png  \n",
            " extracting: /content/image_dataset/CXR533_IM-2140-1001.png  \n",
            " extracting: /content/image_dataset/CXR3881_IM-1969-1001.png  \n",
            " extracting: /content/image_dataset/CXR3258_IM-1544-1001.png  \n",
            " extracting: /content/image_dataset/CXR2383_IM-0941-1001.png  \n",
            " extracting: /content/image_dataset/CXR1643_IM-0421-1001.png  \n",
            " extracting: /content/image_dataset/CXR3678_IM-1831-1001.png  \n",
            " extracting: /content/image_dataset/CXR2619_IM-1108-1001.png  \n",
            " extracting: /content/image_dataset/CXR513_IM-2128-1001.png  \n",
            " extracting: /content/image_dataset/CXR1423_IM-0270-1001.png  \n",
            " extracting: /content/image_dataset/CXR2939_IM-1339-1001.png  \n",
            " extracting: /content/image_dataset/CXR757_IM-2308-1001.png  \n",
            " extracting: /content/image_dataset/CXR443_IM-2078-1001.png  \n",
            " extracting: /content/image_dataset/CXR801_IM-2335-1001.png  \n",
            " extracting: /content/image_dataset/CXR2895_IM-1297-1001.png  \n",
            " extracting: /content/image_dataset/CXR2052_IM-0690-1001.png  \n",
            " extracting: /content/image_dataset/CXR797_IM-2332-1001.png  \n",
            " extracting: /content/image_dataset/CXR3879_IM-1968-1001.png  \n",
            " extracting: /content/image_dataset/CXR116_IM-0107-1001.png  \n",
            " extracting: /content/image_dataset/CXR3931_IM-2003-1001.png  \n",
            " extracting: /content/image_dataset/CXR3708_IM-1852-1001.png  \n",
            " extracting: /content/image_dataset/CXR1386_IM-0246-1001.png  \n",
            " extracting: /content/image_dataset/CXR3696_IM-1846-1001.png  \n",
            " extracting: /content/image_dataset/CXR2271_IM-0860-1001.png  \n",
            " extracting: /content/image_dataset/CXR319_IM-1505-1001.png  \n",
            " extracting: /content/image_dataset/CXR833_IM-2359-1001.png  \n",
            " extracting: /content/image_dataset/CXR2956_IM-1353-1001.png  \n",
            " extracting: /content/image_dataset/CXR1120_IM-0080-1001.png  \n",
            " extracting: /content/image_dataset/CXR2193_IM-0803-1001.png  \n",
            " extracting: /content/image_dataset/CXR735_IM-2294-1001.png  \n",
            " extracting: /content/image_dataset/CXR1321_IM-0207-1001.png  \n",
            " extracting: /content/image_dataset/CXR250_IM-1025-1001.png  \n",
            " extracting: /content/image_dataset/CXR1794_IM-0515-1001.png  \n",
            " extracting: /content/image_dataset/CXR442_IM-2078-1001.png  \n",
            " extracting: /content/image_dataset/CXR610_IM-2197-1001.png  \n",
            " extracting: /content/image_dataset/CXR2331_IM-0900-1001.png  \n",
            " extracting: /content/image_dataset/CXR3090_IM-1445-1001.png  \n",
            " extracting: /content/image_dataset/CXR1154_IM-0104-1001.png  \n",
            " extracting: /content/image_dataset/CXR1854_IM-0555-1001.png  \n",
            " extracting: /content/image_dataset/CXR3811_IM-1921-1001.png  \n",
            " extracting: /content/image_dataset/CXR2161_IM-0779-1001.png  \n",
            " extracting: /content/image_dataset/CXR3813_IM-1922-1001.png  \n",
            " extracting: /content/image_dataset/CXR609_IM-2197-1001.png  \n",
            " extracting: /content/image_dataset/CXR698_IM-2263-1001.png  \n",
            " extracting: /content/image_dataset/CXR353_IM-1726-1001.png  \n",
            " extracting: /content/image_dataset/CXR2053_IM-0691-1001.png  \n",
            " extracting: /content/image_dataset/CXR3778_IM-1894-1001.png  \n",
            " extracting: /content/image_dataset/CXR1922_IM-0598-1001.png  \n",
            " extracting: /content/image_dataset/CXR1123_IM-0080-1001.png  \n",
            " extracting: /content/image_dataset/CXR1762_IM-0497-1001.png  \n",
            " extracting: /content/image_dataset/CXR12_IM-0133-1001.png  \n",
            " extracting: /content/image_dataset/CXR2055_IM-0693-1001.png  \n",
            " extracting: /content/image_dataset/CXR2963_IM-1356-1001.png  \n",
            " extracting: /content/image_dataset/CXR3520_IM-1718-1001.png  \n",
            " extracting: /content/image_dataset/CXR1680_IM-0448-1001.png  \n",
            " extracting: /content/image_dataset/CXR3752_IM-1876-1001.png  \n",
            " extracting: /content/image_dataset/CXR2916_IM-1318-1001.png  \n",
            " extracting: /content/image_dataset/CXR412_IM-2056-1001.png  \n",
            " extracting: /content/image_dataset/CXR2865_IM-1273-1001.png  \n",
            " extracting: /content/image_dataset/CXR1623_IM-0405-1001.png  \n",
            " extracting: /content/image_dataset/CXR2876_IM-1282-1001.png  \n",
            " extracting: /content/image_dataset/CXR400_IM-2050-1001.png  \n",
            " extracting: /content/image_dataset/CXR3041_IM-1415-1001.png  \n",
            " extracting: /content/image_dataset/CXR162_IM-0401-1001.png  \n",
            " extracting: /content/image_dataset/CXR2316_IM-0889-1001.png  \n",
            " extracting: /content/image_dataset/CXR3124_IM-1468-1001.png  \n",
            " extracting: /content/image_dataset/CXR307_IM-1432-1001.png  \n",
            " extracting: /content/image_dataset/CXR3075_IM-1436-1001.png  \n",
            " extracting: /content/image_dataset/CXR3967_IM-2028-1001.png  \n",
            " extracting: /content/image_dataset/CXR1254_IM-0172-1001.png  \n",
            " extracting: /content/image_dataset/CXR2403_IM-0951-1001.png  \n",
            " extracting: /content/image_dataset/CXR732_IM-2292-1001-0002.png  \n",
            " extracting: /content/image_dataset/CXR3100_IM-1452-1001.png  \n",
            " extracting: /content/image_dataset/CXR1310_IM-0202-1001.png  \n",
            " extracting: /content/image_dataset/CXR1766_IM-0500-1001.png  \n",
            " extracting: /content/image_dataset/CXR3050_IM-1420-1001.png  \n",
            " extracting: /content/image_dataset/CXR1324_IM-0209-1001.png  \n",
            " extracting: /content/image_dataset/CXR2731_IM-1189-1001.png  \n",
            " extracting: /content/image_dataset/CXR3385_IM-1632-1001.png  \n",
            " extracting: /content/image_dataset/CXR3690_IM-1841-1001.png  \n",
            " extracting: /content/image_dataset/CXR2400_IM-0950-1001.png  \n",
            " extracting: /content/image_dataset/CXR2361_IM-0926-1001.png  \n",
            " extracting: /content/image_dataset/CXR3588_IM-1766-1001.png  \n",
            " extracting: /content/image_dataset/CXR164_IM-0419-1001.png  \n",
            " extracting: /content/image_dataset/CXR1269_IM-0181-1001.png  \n",
            " extracting: /content/image_dataset/CXR428_IM-2070-1001.png  \n",
            " extracting: /content/image_dataset/CXR1166_IM-0111-1001.png  \n",
            " extracting: /content/image_dataset/CXR1972_IM-0633-1001.png  \n",
            " extracting: /content/image_dataset/CXR1048_IM-0036-1001.png  \n",
            " extracting: /content/image_dataset/CXR1455_IM-0293-1001.png  \n",
            " extracting: /content/image_dataset/CXR2982_IM-1371-1001.png  \n",
            " extracting: /content/image_dataset/CXR586_IM-2182-1001.png  \n",
            " extracting: /content/image_dataset/CXR450_IM-2082-1001.png  \n",
            " extracting: /content/image_dataset/CXR2592_IM-1084-1001.png  \n",
            " extracting: /content/image_dataset/CXR1756_IM-0494-1001.png  \n",
            " extracting: /content/image_dataset/CXR51_IM-2125-1001.png  \n",
            " extracting: /content/image_dataset/CXR44_IM-2078-1001.png  \n",
            " extracting: /content/image_dataset/CXR3721_IM-1859-1001.png  \n",
            " extracting: /content/image_dataset/CXR3322_IM-1588-1001.png  \n",
            " extracting: /content/image_dataset/CXR2774_IM-1215-1001.png  \n",
            " extracting: /content/image_dataset/CXR3356_IM-1610-1001.png  \n",
            " extracting: /content/image_dataset/CXR2722_IM-1184-1001.png  \n",
            " extracting: /content/image_dataset/CXR1418_IM-0267-1001.png  \n",
            " extracting: /content/image_dataset/CXR530_IM-2139-1001.png  \n",
            " extracting: /content/image_dataset/CXR1958_IM-0625-1001.png  \n",
            " extracting: /content/image_dataset/CXR2422_IM-0965-1001.png  \n",
            " extracting: /content/image_dataset/CXR1218_IM-0146-1001.png  \n",
            " extracting: /content/image_dataset/CXR495_IM-2114-1001.png  \n",
            " extracting: /content/image_dataset/CXR3599_IM-1775-1001.png  \n",
            " extracting: /content/image_dataset/CXR737_IM-2295-1001.png  \n",
            " extracting: /content/image_dataset/CXR3414_IM-1650-1001.png  \n",
            " extracting: /content/image_dataset/CXR3505_IM-1707-1001.png  \n",
            " extracting: /content/image_dataset/CXR3423_IM-1656-1001.png  \n",
            " extracting: /content/image_dataset/CXR1709_IM-0467-1001.png  \n",
            " extracting: /content/image_dataset/CXR1506_IM-0330-1001.png  \n",
            " extracting: /content/image_dataset/CXR3539_IM-1731-1001.png  \n",
            " extracting: /content/image_dataset/CXR191_IM-0591-1001.png  \n",
            " extracting: /content/image_dataset/CXR274_IM-1194-1001.png  \n",
            " extracting: /content/image_dataset/CXR878_IM-2392-1001.png  \n",
            " extracting: /content/image_dataset/CXR3481_IM-1692-1001.png  \n",
            " extracting: /content/image_dataset/CXR2644_IM-1130-1001.png  \n",
            " extracting: /content/image_dataset/CXR3753_IM-1877-1001.png  \n",
            " extracting: /content/image_dataset/CXR2208_IM-0815-1001.png  \n",
            " extracting: /content/image_dataset/CXR3370_IM-1622-1001.png  \n",
            " extracting: /content/image_dataset/CXR963_IM-2454-1001.png  \n",
            " extracting: /content/image_dataset/CXR219_IM-0799-1001.png  \n",
            " extracting: /content/image_dataset/CXR644_IM-2223-1001.png  \n",
            " extracting: /content/image_dataset/CXR1007_IM-0008-1001.png  \n",
            " extracting: /content/image_dataset/CXR2847_IM-1256-1001.png  \n",
            " extracting: /content/image_dataset/CXR2831_IM-1249-1001.png  \n",
            " extracting: /content/image_dataset/CXR2134_IM-0756-1001.png  \n",
            " extracting: /content/image_dataset/CXR535_IM-2142-1001.png  \n",
            " extracting: /content/image_dataset/CXR2307_IM-0882-1001.png  \n",
            " extracting: /content/image_dataset/CXR2378_IM-0938-1001.png  \n",
            " extracting: /content/image_dataset/CXR46_IM-2090-1001.png  \n",
            " extracting: /content/image_dataset/CXR3084_IM-1443-1001.png  \n",
            " extracting: /content/image_dataset/CXR187_IM-0563-1001.png  \n",
            " extracting: /content/image_dataset/CXR795_IM-2331-1001.png  \n",
            " extracting: /content/image_dataset/CXR1477_IM-0309-1001.png  \n",
            " extracting: /content/image_dataset/CXR2015_IM-0664-1001.png  \n",
            " extracting: /content/image_dataset/CXR57_IM-2170-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR2667_IM-1146-1001.png  \n",
            " extracting: /content/image_dataset/CXR3086_IM-1444-1001.png  \n",
            " extracting: /content/image_dataset/CXR3620_IM-1791-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR743_IM-2299-1001.png  \n",
            " extracting: /content/image_dataset/CXR1413_IM-0263-1001.png  \n",
            " extracting: /content/image_dataset/CXR349_IM-1697-1001.png  \n",
            " extracting: /content/image_dataset/CXR2756_IM-1205-1001.png  \n",
            " extracting: /content/image_dataset/CXR830_IM-2358-1001.png  \n",
            " extracting: /content/image_dataset/CXR3148_IM-1480-1001.png  \n",
            " extracting: /content/image_dataset/CXR3428_IM-1657-1001.png  \n",
            " extracting: /content/image_dataset/CXR3734_IM-1866-1001.png  \n",
            " extracting: /content/image_dataset/CXR2512_IM-1034-1001.png  \n",
            " extracting: /content/image_dataset/CXR1683_IM-0449-1001.png  \n",
            " extracting: /content/image_dataset/CXR1364_IM-0237-1001.png  \n",
            " extracting: /content/image_dataset/CXR3589_IM-1767-1001.png  \n",
            " extracting: /content/image_dataset/CXR3166_IM-1491-1001.png  \n",
            " extracting: /content/image_dataset/CXR3204_IM-1513-1001.png  \n",
            " extracting: /content/image_dataset/CXR1904_IM-0586-1001.png  \n",
            " extracting: /content/image_dataset/CXR3437_IM-1663-1001.png  \n",
            " extracting: /content/image_dataset/CXR3475_IM-1688-1001.png  \n",
            " extracting: /content/image_dataset/CXR83_IM-2358-1001.png  \n",
            " extracting: /content/image_dataset/CXR3300_IM-1578-1001.png  \n",
            " extracting: /content/image_dataset/CXR3873_IM-1965-1001.png  \n",
            " extracting: /content/image_dataset/CXR2821_IM-1244-1001.png  \n",
            " extracting: /content/image_dataset/CXR1149_IM-0101-1001.png  \n",
            " extracting: /content/image_dataset/CXR3427_IM-1657-1001.png  \n",
            " extracting: /content/image_dataset/CXR3560_IM-1745-1001.png  \n",
            " extracting: /content/image_dataset/CXR778_IM-2321-1001.png  \n",
            " extracting: /content/image_dataset/CXR1335_IM-0215-1001.png  \n",
            " extracting: /content/image_dataset/CXR2370_IM-0931-1001.png  \n",
            " extracting: /content/image_dataset/CXR3422_IM-1656-1001.png  \n",
            " extracting: /content/image_dataset/CXR2462_IM-0995-1001.png  \n",
            " extracting: /content/image_dataset/CXR2585_IM-1082-1001.png  \n",
            " extracting: /content/image_dataset/CXR2388_IM-0944-1001.png  \n",
            " extracting: /content/image_dataset/CXR2238_IM-0835-1001.png  \n",
            " extracting: /content/image_dataset/CXR3531_IM-1726-1001.png  \n",
            " extracting: /content/image_dataset/CXR3777_IM-1894-1001.png  \n",
            " extracting: /content/image_dataset/CXR2730_IM-1189-1001.png  \n",
            " extracting: /content/image_dataset/CXR1226_IM-0150-1001.png  \n",
            " extracting: /content/image_dataset/CXR2871_IM-1277-1001.png  \n",
            " extracting: /content/image_dataset/CXR2466_IM-0997-1001.png  \n",
            " extracting: /content/image_dataset/CXR2002_IM-0654-1001.png  \n",
            " extracting: /content/image_dataset/CXR149_IM-0315-1001.png  \n",
            " extracting: /content/image_dataset/CXR1758_IM-0495-1001.png  \n",
            " extracting: /content/image_dataset/CXR3313_IM-1586-1001.png  \n",
            " extracting: /content/image_dataset/CXR1975_IM-0634-1001.png  \n",
            " extracting: /content/image_dataset/CXR3371_IM-1623-1001.png  \n",
            " extracting: /content/image_dataset/CXR3542_IM-1734-1001.png  \n",
            " extracting: /content/image_dataset/CXR836_IM-2360-1001.png  \n",
            " extracting: /content/image_dataset/CXR1094_IM-0065-1001.png  \n",
            " extracting: /content/image_dataset/CXR518_IM-2131-1001.png  \n",
            " extracting: /content/image_dataset/CXR3366_IM-1618-1001.png  \n",
            " extracting: /content/image_dataset/CXR1967_IM-0629-1001.png  \n",
            " extracting: /content/image_dataset/CXR2382_IM-0941-1001.png  \n",
            " extracting: /content/image_dataset/CXR1819_IM-0530-1001-0002.png  \n",
            " extracting: /content/image_dataset/CXR546_IM-2150-1001.png  \n",
            " extracting: /content/image_dataset/CXR2761_IM-1208-1001.png  \n",
            " extracting: /content/image_dataset/CXR1308_IM-0201-1001.png  \n",
            " extracting: /content/image_dataset/CXR1742_IM-0489-1001.png  \n",
            " extracting: /content/image_dataset/CXR3658_IM-1819-1001.png  \n",
            " extracting: /content/image_dataset/CXR1390_IM-0249-1001.png  \n",
            " extracting: /content/image_dataset/CXR3669_IM-1826-1001.png  \n",
            " extracting: /content/image_dataset/CXR3956_IM-2021-1001.png  \n",
            " extracting: /content/image_dataset/CXR2153_IM-0773-1001.png  \n",
            " extracting: /content/image_dataset/CXR936_IM-2432-1001.png  \n",
            " extracting: /content/image_dataset/CXR3793_IM-1907-1001.png  \n",
            " extracting: /content/image_dataset/CXR820_IM-2351-1001.png  \n",
            " extracting: /content/image_dataset/CXR2905_IM-1309-1001.png  \n",
            " extracting: /content/image_dataset/CXR3082_IM-1441-1001.png  \n",
            " extracting: /content/image_dataset/CXR3417_IM-1652-1001.png  \n",
            " extracting: /content/image_dataset/CXR1049_IM-0036-1001.png  \n",
            " extracting: /content/image_dataset/CXR3516_IM-1715-1001.png  \n",
            " extracting: /content/image_dataset/CXR2189_IM-0798-1001.png  \n",
            " extracting: /content/image_dataset/CXR2514_IM-1036-1001.png  \n",
            " extracting: /content/image_dataset/CXR1710_IM-0469-1001.png  \n",
            " extracting: /content/image_dataset/CXR1748_IM-0490-1001.png  \n",
            " extracting: /content/image_dataset/CXR43_IM-2070-1001.png  \n",
            " extracting: /content/image_dataset/CXR3605_IM-1781-1001.png  \n",
            " extracting: /content/image_dataset/CXR496_IM-2114-1001.png  \n",
            " extracting: /content/image_dataset/CXR112_IM-0080-1001.png  \n",
            " extracting: /content/image_dataset/CXR901_IM-2409-1001.png  \n",
            " extracting: /content/image_dataset/CXR174_IM-0488-1001.png  \n",
            " extracting: /content/image_dataset/CXR1159_IM-0107-1001.png  \n",
            " extracting: /content/image_dataset/CXR2961_IM-1355-1001.png  \n",
            " extracting: /content/image_dataset/CXR2046_IM-0688-1001.png  \n",
            " extracting: /content/image_dataset/CXR3583_IM-1762-1001.png  \n",
            " extracting: /content/image_dataset/CXR2600_IM-1091-1001.png  \n",
            " extracting: /content/image_dataset/CXR1959_IM-0625-1001.png  \n",
            " extracting: /content/image_dataset/CXR922_IM-2423-1001.png  \n",
            " extracting: /content/image_dataset/CXR1811_IM-0525-1001.png  \n",
            " extracting: /content/image_dataset/CXR192_IM-0598-1001.png  \n",
            " extracting: /content/image_dataset/CXR588_IM-2183-1001.png  \n",
            " extracting: /content/image_dataset/CXR606_IM-2195-1001.png  \n",
            " extracting: /content/image_dataset/CXR333_IM-1594-1001.png  \n",
            " extracting: /content/image_dataset/CXR3455_IM-1677-1001.png  \n",
            " extracting: /content/image_dataset/CXR2050_IM-0690-1001.png  \n",
            " extracting: /content/image_dataset/CXR3773_IM-1891-1001.png  \n",
            " extracting: /content/image_dataset/CXR2727_IM-1187-1001.png  \n",
            " extracting: /content/image_dataset/CXR1822_IM-0533-1001.png  \n",
            " extracting: /content/image_dataset/CXR3585_IM-1763-1001.png  \n",
            " extracting: /content/image_dataset/CXR1393_IM-0251-1001.png  \n",
            " extracting: /content/image_dataset/CXR2158_IM-0776-1001.png  \n",
            " extracting: /content/image_dataset/CXR808_IM-2341-1001.png  \n",
            " extracting: /content/image_dataset/CXR604_IM-2193-1001.png  \n",
            " extracting: /content/image_dataset/CXR1264_IM-0179-1001.png  \n",
            " extracting: /content/image_dataset/CXR469_IM-2097-1001.png  \n",
            " extracting: /content/image_dataset/CXR2764_IM-1209-1001.png  \n",
            " extracting: /content/image_dataset/CXR3894_IM-1976-1001.png  \n",
            " extracting: /content/image_dataset/CXR3241_IM-1534-1001.png  \n",
            " extracting: /content/image_dataset/CXR80_IM-2333-1001.png  \n",
            " extracting: /content/image_dataset/CXR2807_IM-1237-1001.png  \n",
            " extracting: /content/image_dataset/CXR124_IM-0161-1001.png  \n",
            " extracting: /content/image_dataset/CXR3049_IM-1420-1001.png  \n",
            " extracting: /content/image_dataset/CXR463_IM-2091-1001.png  \n",
            " extracting: /content/image_dataset/CXR784_IM-2325-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR646_IM-2225-1001.png  \n",
            " extracting: /content/image_dataset/CXR1303_IM-0199-1001-0001.png  \n",
            " extracting: /content/image_dataset/CXR110_IM-0067-1001.png  \n",
            " extracting: /content/image_dataset/CXR1966_IM-0629-1001.png  \n",
            " extracting: /content/image_dataset/CXR3025_IM-1400-1001.png  \n",
            " extracting: /content/image_dataset/CXR2265_IM-0855-1001.png  \n",
            " extracting: /content/image_dataset/CXR285_IM-1258-1001.png  \n",
            " extracting: /content/image_dataset/CXR233_IM-0899-1001.png  \n",
            " extracting: /content/image_dataset/CXR172_IM-0474-1001.png  \n",
            " extracting: /content/image_dataset/CXR3104_IM-1455-1001.png  \n",
            " extracting: /content/image_dataset/CXR618_IM-2201-1001.png  \n",
            " extracting: /content/image_dataset/CXR1047_IM-0036-1001.png  \n",
            " extracting: /content/image_dataset/CXR2171_IM-0786-1001.png  \n",
            " extracting: /content/image_dataset/CXR2549_IM-1057-1001.png  \n",
            " extracting: /content/image_dataset/CXR3515_IM-1715-1001.png  \n",
            " extracting: /content/image_dataset/CXR3079_IM-1438-1001.png  \n",
            " extracting: /content/image_dataset/CXR3689_IM-1840-1001.png  \n",
            " extracting: /content/image_dataset/CXR3316_IM-1586-1001.png  \n",
            " extracting: /content/image_dataset/CXR3460_IM-1681-1001.png  \n",
            " extracting: /content/image_dataset/CXR974_IM-2463-1001.png  \n",
            " extracting: /content/image_dataset/CXR3071_IM-1433-1001.png  \n",
            " extracting: /content/image_dataset/CXR3470_IM-1686-1001.png  \n",
            " extracting: /content/image_dataset/CXR3089_IM-1444-1001.png  \n",
            " extracting: /content/image_dataset/CXR1380_IM-0245-1001.png  \n",
            " extracting: /content/image_dataset/CXR1570_IM-0372-1001.png  \n",
            " extracting: /content/image_dataset/CXR182_IM-0531-1001.png  \n",
            " extracting: /content/image_dataset/CXR2043_IM-0686-1001.png  \n",
            " extracting: /content/image_dataset/CXR3775_IM-1893-1001.png  \n",
            " extracting: /content/image_dataset/CXR2282_IM-0869-1001.png  \n",
            " extracting: /content/image_dataset/CXR189_IM-0578-1001.png  \n",
            " extracting: /content/image_dataset/CXR679_IM-2251-1001.png  \n",
            " extracting: /content/image_dataset/CXR1359_IM-0233-1001.png  \n",
            " extracting: /content/image_dataset/CXR2979_IM-1368-1001-0002.png  \n",
            " extracting: /content/image_dataset/CXR2108_IM-0738-1001.png  \n",
            " extracting: /content/image_dataset/CXR2348_IM-0913-1001.png  \n",
            " extracting: /content/image_dataset/CXR1532_IM-0344-1001.png  \n",
            " extracting: /content/image_dataset/CXR354_IM-1731-1001.png  \n",
            " extracting: /content/image_dataset/CXR1755_IM-0494-1001.png  \n",
            " extracting: /content/image_dataset/CXR3905_IM-1984-1001.png  \n",
            " extracting: /content/image_dataset/CXR653_IM-2230-1001.png  \n",
            " extracting: /content/image_dataset/CXR258_IM-1078-1001.png  \n",
            " extracting: /content/image_dataset/CXR3014_IM-1392-1001.png  \n",
            " extracting: /content/image_dataset/CXR955_IM-2449-1001.png  \n",
            " extracting: /content/image_dataset/CXR145_IM-0290-1001.png  \n",
            " extracting: /content/image_dataset/CXR3112_IM-1461-1001.png  \n",
            " extracting: /content/image_dataset/CXR3410_IM-1648-1001.png  \n",
            " extracting: /content/image_dataset/CXR3703_IM-1850-1001.png  \n",
            " extracting: /content/image_dataset/CXR2131_IM-0755-1001.png  \n",
            " extracting: /content/image_dataset/CXR3066_IM-1430-1001.png  \n",
            " extracting: /content/image_dataset/CXR3553_IM-1741-1001.png  \n",
            " extracting: /content/image_dataset/CXR2856_IM-1263-1001.png  \n",
            " extracting: /content/image_dataset/CXR3325_IM-1591-1001.png  \n",
            " extracting: /content/image_dataset/CXR1750_IM-0493-1001.png  \n",
            " extracting: /content/image_dataset/CXR168_IM-0448-1001.png  \n",
            " extracting: /content/image_dataset/CXR3766_IM-1885-1001.png  \n",
            " extracting: /content/image_dataset/CXR1200_IM-0134-1001.png  \n",
            " extracting: /content/image_dataset/CXR1237_IM-0159-1001.png  \n",
            " extracting: /content/image_dataset/CXR3358_IM-1611-1001.png  \n",
            " extracting: /content/image_dataset/CXR3644_IM-1807-1001.png  \n",
            " extracting: /content/image_dataset/CXR842_IM-2366-1001.png  \n",
            " extracting: /content/image_dataset/CXR3607_IM-1781-1001.png  \n",
            " extracting: /content/image_dataset/CXR2517_IM-1036-1001.png  \n",
            " extracting: /content/image_dataset/CXR828_IM-2357-1001.png  \n",
            " extracting: /content/image_dataset/CXR3401_IM-1645-1001.png  \n",
            " extracting: /content/image_dataset/CXR1513_IM-0333-1001.png  \n",
            " extracting: /content/image_dataset/CXR3142_IM-1477-1001.png  \n",
            " extracting: /content/image_dataset/CXR2234_IM-0833-1001.png  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Define transformations for image preprocessing\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),  # Resize to match input size\n",
        "    transforms.ToTensor(),  # Convert to tensor\n",
        "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])  # Normalize to [0,1]\n",
        "])\n",
        "\n",
        "# Define custom dataset\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert text to fixed-length vector (Simple Embedding)\n",
        "        text_vector = torch.zeros(256, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:256]):\n",
        "            text_vector[i] = ord(char) / 255.0  # Normalize characters\n",
        "\n",
        "        return image, text_vector\n",
        "\n",
        "# Transformer Encoder Block\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "# Vision Transformer (ViT) Block\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, patch_size=8, num_heads=8, num_layers=6, hidden_dim=1024):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.projection = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embed_dim, img_size * img_size * 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous().view(B, self.num_patches, -1)\n",
        "        x = self.projection(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1) + self.pos_embedding.to(x.device)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.fc(x[:, 0]).view(B, 3, H, W)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Full Text-to-Image Model\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super(Text2ImageModel, self).__init__()\n",
        "        self.text_encoder = TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "        self.image_generator = VisionTransformer(embed_dim, img_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        text_features = self.text_encoder(text.unsqueeze(1))\n",
        "        generated_image = self.image_generator(text_features)\n",
        "        return generated_image\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.L1Loss()  # Using L1Loss for stable training\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors in dataloader:\n",
        "            images, text_vectors = images.to(device, dtype=torch.float32), text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_vectors)\n",
        "\n",
        "            loss = criterion(outputs, images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss / len(dataloader)}\")\n",
        "\n",
        "# Example execution\n",
        "batch_size = 16\n",
        "csv_path = \"/content/image_labels_reports.csv\"\n",
        "extract_path = \"/content/image_dataset\"\n",
        "dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64)\n",
        "train_model(model, dataloader, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "VcJmRKq6J9Es",
        "outputId": "81c6e874-61fe-4b4b-aaa4-b88e03c53dc5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "was expecting embedding dimension of 512, but got 256",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-650c6cb95fa3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText2ImageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-4-650c6cb95fa3>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, epochs, lr)\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-650c6cb95fa3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m         \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-650c6cb95fa3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mattn_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mattn_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mfc_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/activation.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1366\u001b[0m             )\n\u001b[1;32m   1367\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1368\u001b[0;31m             attn_output, attn_output_weights = F.multi_head_attention_forward(\n\u001b[0m\u001b[1;32m   1369\u001b[0m                 \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1370\u001b[0m                 \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mmulti_head_attention_forward\u001b[0;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   6068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6069\u001b[0m     assert (\n\u001b[0;32m-> 6070\u001b[0;31m         \u001b[0membed_dim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0membed_dim_to_check\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6071\u001b[0m     ), f\"was expecting embedding dimension of {embed_dim_to_check}, but got {embed_dim}\"\n\u001b[1;32m   6072\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAssertionError\u001b[0m: was expecting embedding dimension of 512, but got 256"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "# Set CUDA_LAUNCH_BLOCKING for debugging\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "# Define transformations for image preprocessing\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
        "])\n",
        "\n",
        "# Define custom dataset\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.embed_dim = 512  # Match model's embedding dimension\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        # Load and preprocess image\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Convert text to fixed-length vector with correct embedding dimension\n",
        "        text_vector = torch.zeros(self.embed_dim, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:self.embed_dim]):\n",
        "            text_vector[i] = ord(char) / 255.0\n",
        "\n",
        "        return image, text_vector\n",
        "\n",
        "# Transformer Encoder Block\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super(TransformerEncoderBlock, self).__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "# Vision Transformer (ViT) Block\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, patch_size=8, num_heads=8, num_layers=6, hidden_dim=1024):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.projection = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.fc = nn.Linear(embed_dim, img_size * img_size * 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous().view(B, self.num_patches, -1)\n",
        "        x = self.projection(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1) + self.pos_embedding.to(x.device)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        x = self.fc(x[:, 0]).view(B, 3, H, W)\n",
        "        return torch.sigmoid(x)\n",
        "\n",
        "# Full Text-to-Image Model\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super(Text2ImageModel, self).__init__()\n",
        "        self.text_encoder = TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "        self.image_generator = VisionTransformer(embed_dim, img_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # Ensure text has the right shape for the transformer [batch_size, seq_len, embed_dim]\n",
        "        text_features = self.text_encoder(text.unsqueeze(1))\n",
        "        generated_image = self.image_generator(text_features)\n",
        "        return generated_image\n",
        "\n",
        "# Training function\n",
        "def train_model(model, dataloader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors in dataloader:\n",
        "            images = images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_vectors)\n",
        "            loss = criterion(outputs, images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "# Example execution\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 16\n",
        "    csv_path = \"/content/image_labels_reports.csv\"\n",
        "    extract_path = \"/content/image_dataset\"\n",
        "\n",
        "    dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64)\n",
        "    train_model(model, dataloader, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "id": "UC-QcDKnKc0u",
        "outputId": "cea0efcd-8696-4953-a4e3-ef4c86c2807e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "not enough values to unpack (expected 4, got 3)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-c8d590efb735>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText2ImageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-5-c8d590efb735>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, epochs, lr)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_vectors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c8d590efb735>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0;31m# Ensure text has the right shape for the transformer [batch_size, seq_len, embed_dim]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mtext_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mgenerated_image\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mgenerated_image\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-c8d590efb735>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munfold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_patches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 3)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
        "])\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.embed_dim = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_vector = torch.zeros(self.embed_dim, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:self.embed_dim]):\n",
        "            text_vector[i] = ord(char) / 255.0\n",
        "\n",
        "        return image, text_vector\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "class ImageGenerator(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "\n",
        "        # Project text features to initial spatial representation\n",
        "        self.initial_projection = nn.Linear(embed_dim, 256 * 4 * 4)  # Start with 4x4 feature map\n",
        "\n",
        "        # Upsampling layers to generate final image\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()  # Output in range [0,1]\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, embed_dim]\n",
        "        x = x[:, 0, :]  # Take first token [batch_size, embed_dim]\n",
        "        x = self.initial_projection(x)  # [batch_size, 256*4*4]\n",
        "        x = x.view(-1, 256, 4, 4)  # [batch_size, 256, 4, 4]\n",
        "        x = self.decoder(x)  # [batch_size, 3, 64, 64]\n",
        "        return x\n",
        "\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super().__init__()\n",
        "        # Text encoder\n",
        "        self.text_encoder = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        # Image generator\n",
        "        self.image_generator = ImageGenerator(embed_dim, img_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # text: [batch_size, embed_dim]\n",
        "        x = text.unsqueeze(1)  # [batch_size, 1, embed_dim]\n",
        "        for layer in self.text_encoder:\n",
        "            x = layer(x)\n",
        "        generated_image = self.image_generator(x)\n",
        "        return generated_image\n",
        "\n",
        "def train_model(model, dataloader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors in dataloader:\n",
        "            images = images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_vectors)\n",
        "            loss = criterion(outputs, images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 16\n",
        "    csv_path = \"/content/image_labels_reports.csv\"\n",
        "    extract_path = \"/content/image_dataset\"\n",
        "\n",
        "    dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64)\n",
        "    train_model(model, dataloader, epochs=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dfpkg_nOKmLw",
        "outputId": "4e07861f-f0f2-47b6-8e9e-41e3b8abb350"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Loss: 0.1524\n",
            "Epoch 2/10, Loss: 0.1387\n",
            "Epoch 3/10, Loss: 0.1381\n",
            "Epoch 4/10, Loss: 0.1383\n",
            "Epoch 5/10, Loss: 0.1377\n",
            "Epoch 6/10, Loss: 0.1378\n",
            "Epoch 7/10, Loss: 0.1376\n",
            "Epoch 8/10, Loss: 0.1378\n",
            "Epoch 9/10, Loss: 0.1367\n",
            "Epoch 10/10, Loss: 0.1372\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision.utils as vutils  # For saving images\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
        "])\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.embed_dim = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_vector = torch.zeros(self.embed_dim, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:self.embed_dim]):\n",
        "            text_vector[i] = ord(char) / 255.0\n",
        "\n",
        "        return image, text_vector\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "class ImageGenerator(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.initial_projection = nn.Linear(embed_dim, 256 * 4 * 4)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0, :]\n",
        "        x = self.initial_projection(x)\n",
        "        x = x.view(-1, 256, 4, 4)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super().__init__()\n",
        "        self.text_encoder = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.image_generator = ImageGenerator(embed_dim, img_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        x = text.unsqueeze(1)\n",
        "        for layer in self.text_encoder:\n",
        "            x = layer(x)\n",
        "        generated_image = self.image_generator(x)\n",
        "        return generated_image\n",
        "\n",
        "def train_model(model, dataloader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors in dataloader:\n",
        "            images = images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(text_vectors)\n",
        "            loss = criterion(outputs, images)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model  # Return trained model\n",
        "\n",
        "def test_model(model, test_dataloader, save_dir=\"generated_images\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    total_test_loss = 0\n",
        "\n",
        "    # Create directory for saving images if it doesn't exist\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (real_images, text_vectors) in enumerate(test_dataloader):\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            # Generate images from text\n",
        "            generated_images = model(text_vectors)\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(generated_images, real_images)\n",
        "            total_test_loss += loss.item()\n",
        "\n",
        "            # Save first batch of generated images for inspection\n",
        "            if i == 0:\n",
        "                # Denormalize images for saving (assuming input was normalized to [0,1])\n",
        "                generated_images_denorm = generated_images\n",
        "                real_images_denorm = real_images\n",
        "\n",
        "                # Save real vs generated comparison\n",
        "                comparison = torch.cat([real_images_denorm[:8], generated_images_denorm[:8]])\n",
        "                vutils.save_image(\n",
        "                    comparison,\n",
        "                    os.path.join(save_dir, 'real_vs_generated.png'),\n",
        "                    nrow=8,\n",
        "                    normalize=True\n",
        "                )\n",
        "\n",
        "                print(f\"Saved sample images to {save_dir}/real_vs_generated.png\")\n",
        "                print(\"Top row: Real images, Bottom row: Generated images\")\n",
        "\n",
        "    avg_test_loss = total_test_loss / len(test_dataloader)\n",
        "    print(f\"Test Loss: {avg_test_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 16\n",
        "    csv_path = \"/content/image_labels_reports.csv\"\n",
        "    extract_path = \"/content/image_dataset\"\n",
        "\n",
        "    # Full dataset\n",
        "    full_dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "\n",
        "    # Split into train and test\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Initialize and train model\n",
        "    model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64)\n",
        "    trained_model = train_model(model, train_dataloader, epochs=10)\n",
        "\n",
        "    # Test the model\n",
        "    test_model(trained_model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulqsc4vRKxM1",
        "outputId": "e9583067-08cf-481a-992d-0567526675f0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.1548\n",
            "Epoch 2/10, Train Loss: 0.1388\n",
            "Epoch 3/10, Train Loss: 0.1395\n",
            "Epoch 4/10, Train Loss: 0.1393\n",
            "Epoch 5/10, Train Loss: 0.1383\n",
            "Epoch 6/10, Train Loss: 0.1376\n",
            "Epoch 7/10, Train Loss: 0.1392\n",
            "Epoch 8/10, Train Loss: 0.1382\n",
            "Epoch 9/10, Train Loss: 0.1387\n",
            "Epoch 10/10, Train Loss: 0.1379\n",
            "Saved sample images to generated_images/real_vs_generated.png\n",
            "Top row: Real images, Bottom row: Generated images\n",
            "Test Loss: 0.1330\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision.utils as vutils\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
        "])\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.embed_dim = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_vector = torch.zeros(self.embed_dim, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:self.embed_dim]):\n",
        "            text_vector[i] = ord(char) / 255.0\n",
        "\n",
        "        return image, text_vector\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, patch_size=8, num_heads=8, num_layers=6, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.projection = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous().view(B, self.num_patches, -1)\n",
        "        x = self.projection(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1) + self.pos_embedding.to(x.device)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return self.norm(x[:, 0])  # Return CLS token embedding\n",
        "\n",
        "class ImageGenerator(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.initial_projection = nn.Linear(embed_dim, 256 * 4 * 4)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0, :]\n",
        "        x = self.initial_projection(x)\n",
        "        x = x.view(-1, 256, 4, 4)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super().__init__()\n",
        "        self.text_encoder = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.image_generator = ImageGenerator(embed_dim, img_size)\n",
        "        self.vit_encoder = VisionTransformer(embed_dim, img_size)  # ViT for real images\n",
        "\n",
        "    def forward(self, text, target_img=None):\n",
        "        x = text.unsqueeze(1)\n",
        "        for layer in self.text_encoder:\n",
        "            x = layer(x)\n",
        "\n",
        "        generated_image = self.image_generator(x)\n",
        "\n",
        "        if target_img is not None:\n",
        "            vit_features = self.vit_encoder(target_img)\n",
        "            return generated_image, vit_features\n",
        "        return generated_image\n",
        "\n",
        "def train_model(model, dataloader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    image_criterion = nn.L1Loss()\n",
        "    feature_criterion = nn.MSELoss()  # For ViT features\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors in dataloader:\n",
        "            images = images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            generated_images, vit_features = model(text_vectors, images)\n",
        "\n",
        "            # Combined loss: image reconstruction + feature similarity\n",
        "            image_loss = image_criterion(generated_images, images)\n",
        "            text_features = text_vectors  # Use raw text features for simplicity\n",
        "            feature_loss = feature_criterion(vit_features, text_features)\n",
        "            loss = image_loss + 0.1 * feature_loss  # Weight feature loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_model(model, test_dataloader, save_dir=\"generated_images\"):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    image_criterion = nn.L1Loss()\n",
        "    feature_criterion = nn.MSELoss()\n",
        "    total_image_loss = 0\n",
        "    total_feature_loss = 0\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (real_images, text_vectors) in enumerate(test_dataloader):\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            generated_images, vit_features = model(text_vectors, real_images)\n",
        "\n",
        "            image_loss = image_criterion(generated_images, real_images)\n",
        "            feature_loss = feature_criterion(vit_features, text_vectors)\n",
        "            total_image_loss += image_loss.item()\n",
        "            total_feature_loss += feature_loss.item()\n",
        "\n",
        "            if i == 0:\n",
        "                comparison = torch.cat([real_images[:8], generated_images[:8]])\n",
        "                vutils.save_image(\n",
        "                    comparison,\n",
        "                    os.path.join(save_dir, 'real_vs_generated.png'),\n",
        "                    nrow=8,\n",
        "                    normalize=True\n",
        "                )\n",
        "                print(f\"Saved sample images to {save_dir}/real_vs_generated.png\")\n",
        "                print(\"Top row: Real images, Bottom row: Generated images\")\n",
        "\n",
        "    avg_image_loss = total_image_loss / len(test_dataloader)\n",
        "    avg_feature_loss = total_feature_loss / len(test_dataloader)\n",
        "    print(f\"Test Image Loss: {avg_image_loss:.4f}\")\n",
        "    print(f\"Test Feature Loss: {avg_feature_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 16\n",
        "    csv_path = \"/content/image_labels_reports.csv\"\n",
        "    extract_path = \"/content/image_dataset\"\n",
        "\n",
        "    full_dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64)\n",
        "    trained_model = train_model(model, train_dataloader, epochs=10)\n",
        "    test_model(trained_model, test_dataloader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22_OQ2fsLC-a",
        "outputId": "a90211bc-51ce-4769-e588-ca49b732235f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.2345\n",
            "Epoch 2/10, Train Loss: 0.2082\n",
            "Epoch 3/10, Train Loss: 0.1979\n",
            "Epoch 4/10, Train Loss: 0.1889\n",
            "Epoch 5/10, Train Loss: 0.1808\n",
            "Epoch 6/10, Train Loss: 0.1752\n",
            "Epoch 7/10, Train Loss: 0.1673\n",
            "Epoch 8/10, Train Loss: 0.1607\n",
            "Epoch 9/10, Train Loss: 0.1546\n",
            "Epoch 10/10, Train Loss: 0.1506\n",
            "Saved sample images to generated_images/real_vs_generated.png\n",
            "Top row: Real images, Bottom row: Generated images\n",
            "Test Image Loss: 0.1415\n",
            "Test Feature Loss: 0.1139\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "import torchvision.utils as vutils\n",
        "from torchvision.utils import make_grid\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
        "])\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.embed_dim = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_vector = torch.zeros(self.embed_dim, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:self.embed_dim]):\n",
        "            text_vector[i] = ord(char) / 255.0\n",
        "\n",
        "        return image, text_vector, text  # Return text string as well\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, patch_size=8, num_heads=8, num_layers=6, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.projection = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous().view(B, self.num_patches, -1)\n",
        "        x = self.projection(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1) + self.pos_embedding.to(x.device)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return self.norm(x[:, 0])\n",
        "\n",
        "class ImageGenerator(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.initial_projection = nn.Linear(embed_dim, 256 * 4 * 4)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0, :]\n",
        "        x = self.initial_projection(x)\n",
        "        x = x.view(-1, 256, 4, 4)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super().__init__()\n",
        "        self.text_encoder = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.image_generator = ImageGenerator(embed_dim, img_size)\n",
        "        self.vit_encoder = VisionTransformer(embed_dim, img_size)\n",
        "\n",
        "    def forward(self, text, target_img=None):\n",
        "        x = text.unsqueeze(1)\n",
        "        for layer in self.text_encoder:\n",
        "            x = layer(x)\n",
        "\n",
        "        generated_image = self.image_generator(x)\n",
        "\n",
        "        if target_img is not None:\n",
        "            vit_features = self.vit_encoder(target_img)\n",
        "            return generated_image, vit_features\n",
        "        return generated_image\n",
        "\n",
        "def train_model(model, dataloader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    image_criterion = nn.L1Loss()\n",
        "    feature_criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors, _ in dataloader:  # Ignore text string here\n",
        "            images = images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            generated_images, vit_features = model(text_vectors, images)\n",
        "\n",
        "            image_loss = image_criterion(generated_images, images)\n",
        "            text_features = text_vectors\n",
        "            feature_loss = feature_criterion(vit_features, text_features)\n",
        "            loss = image_loss + 0.1 * feature_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_model(model, test_dataloader, save_dir=\"generated_images\", num_samples=8):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    image_criterion = nn.L1Loss()\n",
        "    feature_criterion = nn.MSELoss()\n",
        "    total_image_loss = 0\n",
        "    total_feature_loss = 0\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Collect samples from the first batch\n",
        "        for i, (real_images, text_vectors, texts) in enumerate(test_dataloader):\n",
        "            if i >= 1:  # Only process first batch for visualization\n",
        "                break\n",
        "\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            generated_images, vit_features = model(text_vectors, real_images)\n",
        "\n",
        "            image_loss = image_criterion(generated_images, real_images)\n",
        "            feature_loss = feature_criterion(vit_features, text_vectors)\n",
        "            total_image_loss += image_loss.item()\n",
        "            total_feature_loss += feature_loss.item()\n",
        "\n",
        "            # Save images and text for the first batch (up to num_samples)\n",
        "            num_to_save = min(num_samples, real_images.size(0))\n",
        "            real_images_sample = real_images[:num_to_save]\n",
        "            generated_images_sample = generated_images[:num_to_save]\n",
        "            texts_sample = texts[:num_to_save]\n",
        "\n",
        "            # Save comparison image\n",
        "            comparison = torch.cat([real_images_sample, generated_images_sample])\n",
        "            vutils.save_image(\n",
        "                comparison,\n",
        "                os.path.join(save_dir, 'real_vs_generated.png'),\n",
        "                nrow=num_to_save,\n",
        "                normalize=True,\n",
        "                padding=10\n",
        "            )\n",
        "            print(f\"Saved comparison image to {save_dir}/real_vs_generated.png\")\n",
        "            print(\"Top row: Real images, Bottom row: Generated images\")\n",
        "\n",
        "            # Save individual images with text descriptions\n",
        "            for idx in range(num_to_save):\n",
        "                # Save real image\n",
        "                vutils.save_image(\n",
        "                    real_images_sample[idx],\n",
        "                    os.path.join(save_dir, f'real_image_{idx}.png'),\n",
        "                    normalize=True\n",
        "                )\n",
        "                # Save generated image\n",
        "                vutils.save_image(\n",
        "                    generated_images_sample[idx],\n",
        "                    os.path.join(save_dir, f'generated_image_{idx}.png'),\n",
        "                    normalize=True\n",
        "                )\n",
        "                # Save text description\n",
        "                with open(os.path.join(save_dir, f'description_{idx}.txt'), 'w') as f:\n",
        "                    f.write(texts_sample[idx])\n",
        "\n",
        "            print(f\"Saved {num_to_save} individual real images, generated images, and text descriptions to {save_dir}\")\n",
        "\n",
        "        # Calculate average losses across entire test set\n",
        "        for real_images, text_vectors, _ in test_dataloader:\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "            generated_images, vit_features = model(text_vectors, real_images)\n",
        "            total_image_loss += image_criterion(generated_images, real_images).item()\n",
        "            total_feature_loss += feature_criterion(vit_features, text_vectors).item()\n",
        "\n",
        "    avg_image_loss = total_image_loss / len(test_dataloader)\n",
        "    avg_feature_loss = total_feature_loss / len(test_dataloader)\n",
        "    print(f\"Test Image Loss: {avg_image_loss:.4f}\")\n",
        "    print(f\"Test Feature Loss: {avg_feature_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 16\n",
        "    csv_path = \"/content/image_labels_reports.csv\"\n",
        "    extract_path = \"/content/image_dataset\"\n",
        "\n",
        "    full_dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64)\n",
        "    trained_model = train_model(model, train_dataloader, epochs=10)\n",
        "    test_model(trained_model, test_dataloader, num_samples=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg9zhw7sLn6s",
        "outputId": "52cf6912-0083-45a3-98f0-859b3435a708"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Train Loss: 0.2394\n",
            "Epoch 2/10, Train Loss: 0.2098\n",
            "Epoch 3/10, Train Loss: 0.1984\n",
            "Epoch 4/10, Train Loss: 0.1882\n",
            "Epoch 5/10, Train Loss: 0.1845\n",
            "Epoch 6/10, Train Loss: 0.1743\n",
            "Epoch 7/10, Train Loss: 0.1681\n",
            "Epoch 8/10, Train Loss: 0.1613\n",
            "Epoch 9/10, Train Loss: 0.1567\n",
            "Epoch 10/10, Train Loss: 0.1514\n",
            "Saved comparison image to generated_images/real_vs_generated.png\n",
            "Top row: Real images, Bottom row: Generated images\n",
            "Saved 8 individual real images, generated images, and text descriptions to generated_images\n",
            "Test Image Loss: 0.1511\n",
            "Test Feature Loss: 0.1207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/generated_images"
      ],
      "metadata": {
        "id": "EWhbytJDMIlG"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import pandas as pd\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
        "])\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.embed_dim = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_vector = torch.zeros(self.embed_dim, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:self.embed_dim]):\n",
        "            text_vector[i] = ord(char) / 255.0\n",
        "\n",
        "        return image, text_vector, text\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, patch_size=8, num_heads=8, num_layers=6, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.projection = nn.Linear(patch_size * patch_size * 3, embed_dim)\n",
        "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches + 1, embed_dim))\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous().view(B, self.num_patches, -1)\n",
        "        x = self.projection(x)\n",
        "\n",
        "        cls_tokens = self.cls_token.repeat(B, 1, 1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1) + self.pos_embedding.to(x.device)\n",
        "\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        return self.norm(x[:, 0])\n",
        "\n",
        "class ImageGenerator(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.initial_projection = nn.Linear(embed_dim, 256 * 4 * 4)\n",
        "        self.decoder = nn.Sequential(\n",
        "            nn.ConvTranspose2d(256, 128, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(128, 64, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(64, 32, 4, stride=2, padding=1),\n",
        "            nn.ReLU(),\n",
        "            nn.ConvTranspose2d(32, 3, 4, stride=2, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x[:, 0, :]\n",
        "        x = self.initial_projection(x)\n",
        "        x = x.view(-1, 256, 4, 4)\n",
        "        x = self.decoder(x)\n",
        "        return x\n",
        "\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super().__init__()\n",
        "        self.text_encoder = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.image_generator = ImageGenerator(embed_dim, img_size)\n",
        "        self.vit_encoder = VisionTransformer(embed_dim, img_size)\n",
        "\n",
        "    def forward(self, text, target_img=None):\n",
        "        x = text.unsqueeze(1)\n",
        "        for layer in self.text_encoder:\n",
        "            x = layer(x)\n",
        "\n",
        "        generated_image = self.image_generator(x)\n",
        "\n",
        "        if target_img is not None:\n",
        "            vit_features = self.vit_encoder(target_img)\n",
        "            return generated_image, vit_features\n",
        "        return generated_image\n",
        "\n",
        "def train_model(model, dataloader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    image_criterion = nn.L1Loss()\n",
        "    feature_criterion = nn.MSELoss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors, _ in dataloader:\n",
        "            images = images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            generated_images, vit_features = model(text_vectors, images)\n",
        "\n",
        "            image_loss = image_criterion(generated_images, images)\n",
        "            text_features = text_vectors\n",
        "            feature_loss = feature_criterion(vit_features, text_features)\n",
        "            loss = image_loss + 0.1 * feature_loss\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_model(model, test_dataloader, save_dir=\"generated_images\", num_samples=8):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    image_criterion = nn.L1Loss()\n",
        "    feature_criterion = nn.MSELoss()\n",
        "    total_image_loss = 0\n",
        "    total_feature_loss = 0\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (real_images, text_vectors, texts) in enumerate(test_dataloader):\n",
        "            if i >= 1:  # Process only first batch for visualization\n",
        "                break\n",
        "\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            generated_images, vit_features = model(text_vectors, real_images)\n",
        "\n",
        "            image_loss = image_criterion(generated_images, real_images)\n",
        "            feature_loss = feature_criterion(vit_features, text_vectors)\n",
        "            total_image_loss += image_loss.item()\n",
        "            total_feature_loss += feature_loss.item()\n",
        "\n",
        "            # Convert images to PIL for combining with text\n",
        "            num_to_save = min(num_samples, real_images.size(0))\n",
        "            real_images_np = real_images[:num_to_save].cpu().numpy().transpose(0, 2, 3, 1)  # [B, H, W, C]\n",
        "            generated_images_np = generated_images[:num_to_save].cpu().numpy().transpose(0, 2, 3, 1)\n",
        "            texts_sample = texts[:num_to_save]\n",
        "\n",
        "            # Create side-by-side visualization for each sample\n",
        "            for idx in range(num_to_save):\n",
        "                # Convert tensors to PIL images\n",
        "                real_img = Image.fromarray((real_images_np[idx] * 255).astype(np.uint8))\n",
        "                gen_img = Image.fromarray((generated_images_np[idx] * 255).astype(np.uint8))\n",
        "\n",
        "                # Create text image\n",
        "                text_img = Image.new('RGB', (200, 64), color=(255, 255, 255))  # White background\n",
        "                draw = ImageDraw.Draw(text_img)\n",
        "                try:\n",
        "                    font = ImageFont.truetype(\"arial.ttf\", 12)\n",
        "                except:\n",
        "                    font = ImageFont.load_default()  # Fallback if arial isn't available\n",
        "                text = texts_sample[idx][:50] + \"...\" if len(texts_sample[idx]) > 50 else texts_sample[idx]  # Truncate long text\n",
        "                draw.text((5, 5), text, font=font, fill=(0, 0, 0))  # Black text\n",
        "\n",
        "                # Combine images horizontally: text | real | generated\n",
        "                combined_width = text_img.width + real_img.width + gen_img.width\n",
        "                combined_img = Image.new('RGB', (combined_width, 64))\n",
        "                combined_img.paste(text_img, (0, 0))\n",
        "                combined_img.paste(real_img, (text_img.width, 0))\n",
        "                combined_img.paste(gen_img, (text_img.width + real_img.width, 0))\n",
        "\n",
        "                # Save combined image\n",
        "                combined_img.save(os.path.join(save_dir, f'sample_{idx}_text_real_gen.png'))\n",
        "\n",
        "            print(f\"Saved {num_to_save} samples (text | real | generated) to {save_dir}\")\n",
        "\n",
        "        # Calculate average losses across entire test set\n",
        "        for real_images, text_vectors, _ in test_dataloader:\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "            generated_images, vit_features = model(text_vectors, real_images)\n",
        "            total_image_loss += image_criterion(generated_images, real_images).item()\n",
        "            total_feature_loss += feature_criterion(vit_features, text_vectors).item()\n",
        "\n",
        "    avg_image_loss = total_image_loss / len(test_dataloader)\n",
        "    avg_feature_loss = total_feature_loss / len(test_dataloader)\n",
        "    print(f\"Test Image Loss: {avg_image_loss:.4f}\")\n",
        "    print(f\"Test Feature Loss: {avg_feature_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 16\n",
        "    csv_path = \"/content/image_labels_reports.csv\"\n",
        "    extract_path = \"/content/image_dataset\"\n",
        "\n",
        "    full_dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64)\n",
        "    trained_model = train_model(model, train_dataloader, epochs=100)\n",
        "    test_model(trained_model, test_dataloader, num_samples=8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 766
        },
        "id": "I_fCG1mZMltx",
        "outputId": "ae3c2eec-9eb7-47f7-ece5-8f06271daf9e"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Train Loss: 0.2366\n",
            "Epoch 2/100, Train Loss: 0.2076\n",
            "Epoch 3/100, Train Loss: 0.1983\n",
            "Epoch 4/100, Train Loss: 0.1902\n",
            "Epoch 5/100, Train Loss: 0.1816\n",
            "Epoch 6/100, Train Loss: 0.1745\n",
            "Epoch 7/100, Train Loss: 0.1701\n",
            "Epoch 8/100, Train Loss: 0.1619\n",
            "Epoch 9/100, Train Loss: 0.1562\n",
            "Epoch 10/100, Train Loss: 0.1515\n",
            "Epoch 11/100, Train Loss: 0.1473\n",
            "Epoch 12/100, Train Loss: 0.1448\n",
            "Epoch 13/100, Train Loss: 0.1436\n",
            "Epoch 14/100, Train Loss: 0.1415\n",
            "Epoch 15/100, Train Loss: 0.1419\n",
            "Epoch 16/100, Train Loss: 0.1420\n",
            "Epoch 17/100, Train Loss: 0.1406\n",
            "Epoch 18/100, Train Loss: 0.1402\n",
            "Epoch 19/100, Train Loss: 0.1410\n",
            "Epoch 20/100, Train Loss: 0.1404\n",
            "Epoch 21/100, Train Loss: 0.1399\n",
            "Epoch 22/100, Train Loss: 0.1401\n",
            "Epoch 23/100, Train Loss: 0.1412\n",
            "Epoch 24/100, Train Loss: 0.1402\n",
            "Epoch 25/100, Train Loss: 0.1397\n",
            "Epoch 26/100, Train Loss: 0.1415\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-7d01558a9507>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mText2ImageModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membed_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_heads\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 258\u001b[0;31m     \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    259\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-7d01558a9507>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, dataloader, epochs, lr)\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mavg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import pandas as pd\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.0, 0.0, 0.0], std=[1.0, 1.0, 1.0])\n",
        "])\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.embed_dim = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_vector = torch.zeros(self.embed_dim, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:self.embed_dim]):\n",
        "            text_vector[i] = ord(char) / 255.0\n",
        "\n",
        "        return image, text_vector, text\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "class VisionTransformerGenerator(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, patch_size=8, num_heads=8, num_layers=6, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Input projection from text embedding to transformer dimension\n",
        "        self.text_projection = nn.Linear(embed_dim, embed_dim)\n",
        "\n",
        "        # Positional embedding for patches\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
        "\n",
        "        # Transformer layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection to image patches\n",
        "        self.patch_projection = nn.Linear(embed_dim, patch_size * patch_size * 3)\n",
        "\n",
        "        # Final convolution to refine the output\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 3, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, embed_dim], where seq_len=1 from text encoder\n",
        "        B = x.size(0)\n",
        "        x = self.text_projection(x[:, 0, :])  # [batch_size, embed_dim]\n",
        "\n",
        "        # Expand text features to match number of patches\n",
        "        x = x.unsqueeze(1).repeat(1, self.num_patches, 1)  # [batch_size, num_patches, embed_dim]\n",
        "        x = x + self.pos_embedding.to(x.device)\n",
        "\n",
        "        # Process through transformer layers\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Project to patch-sized image fragments\n",
        "        x = self.patch_projection(x)  # [batch_size, num_patches, patch_size*patch_size*3]\n",
        "\n",
        "        # Reshape to image format\n",
        "        x = x.view(B, self.num_patches, 3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 4, 1)  # [batch_size, 3, patch_size, patch_size, num_patches]\n",
        "\n",
        "        # Reshape patches into full image\n",
        "        grid_size = int(self.num_patches ** 0.5)  # Assuming square grid\n",
        "        x = x.reshape(B, 3, self.patch_size, grid_size, self.patch_size, grid_size)\n",
        "        x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
        "        x = x.view(B, 3, self.img_size, self.img_size)\n",
        "\n",
        "        # Final refinement\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super().__init__()\n",
        "        self.text_encoder = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.vit_generator = VisionTransformerGenerator(embed_dim, img_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # Text encoding\n",
        "        x = text.unsqueeze(1)  # [batch_size, 1, embed_dim]\n",
        "        for layer in self.text_encoder:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Generate image using ViT\n",
        "        generated_image = self.vit_generator(x)\n",
        "        return generated_image\n",
        "\n",
        "def train_model(model, dataloader, epochs=5, lr=0.001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors, _ in dataloader:\n",
        "            images = images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            generated_images = model(text_vectors)\n",
        "            loss = criterion(generated_images, images)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_model(model, test_dataloader, save_dir=\"generated_images\", num_samples=8):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    total_loss = 0\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (real_images, text_vectors, texts) in enumerate(test_dataloader):\n",
        "            if i >= 1:  # Process only first batch for visualization\n",
        "                break\n",
        "\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            generated_images = model(text_vectors)\n",
        "\n",
        "            loss = criterion(generated_images, real_images)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert images to PIL for combining with text\n",
        "            num_to_save = min(num_samples, real_images.size(0))\n",
        "            real_images_np = real_images[:num_to_save].cpu().numpy().transpose(0, 2, 3, 1)\n",
        "            generated_images_np = generated_images[:num_to_save].cpu().numpy().transpose(0, 2, 3, 1)\n",
        "            texts_sample = texts[:num_to_save]\n",
        "\n",
        "            # Create side-by-side visualization for each sample\n",
        "            for idx in range(num_to_save):\n",
        "                real_img = Image.fromarray((real_images_np[idx] * 255).astype(np.uint8))\n",
        "                gen_img = Image.fromarray((generated_images_np[idx] * 255).astype(np.uint8))\n",
        "\n",
        "                text_img = Image.new('RGB', (200, 64), color=(255, 255, 255))\n",
        "                draw = ImageDraw.Draw(text_img)\n",
        "                try:\n",
        "                    font = ImageFont.truetype(\"arial.ttf\", 12)\n",
        "                except:\n",
        "                    font = ImageFont.load_default()\n",
        "                text = texts_sample[idx][:50] + \"...\" if len(texts_sample[idx]) > 50 else texts_sample[idx]\n",
        "                draw.text((5, 5), text, font=font, fill=(0, 0, 0))\n",
        "\n",
        "                combined_width = text_img.width + real_img.width + gen_img.width\n",
        "                combined_img = Image.new('RGB', (combined_width, 64))\n",
        "                combined_img.paste(text_img, (0, 0))\n",
        "                combined_img.paste(real_img, (text_img.width, 0))\n",
        "                combined_img.paste(gen_img, (text_img.width + real_img.width, 0))\n",
        "\n",
        "                combined_img.save(os.path.join(save_dir, f'sample_{idx}_text_real_gen.png'))\n",
        "\n",
        "            print(f\"Saved {num_to_save} samples (text | real | generated) to {save_dir}\")\n",
        "\n",
        "        # Calculate average loss across entire test set\n",
        "        for real_images, text_vectors, _ in test_dataloader:\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "            generated_images = model(text_vectors)\n",
        "            total_loss += criterion(generated_images, real_images).item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_dataloader)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 16\n",
        "    csv_path = \"/content/image_labels_reports.csv\"\n",
        "    extract_path = \"/content/image_dataset\"\n",
        "\n",
        "    full_dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64)\n",
        "    trained_model = train_model(model, train_dataloader, epochs=10)\n",
        "    test_model(trained_model, test_dataloader, num_samples=8)"
      ],
      "metadata": {
        "id": "r4V1rWWGMrcS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Worked"
      ],
      "metadata": {
        "id": "P98LszhCSoNi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "import pandas as pd\n",
        "import torchvision.utils as vutils\n",
        "import numpy as np\n",
        "\n",
        "os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
        "\n",
        "image_transforms = transforms.Compose([\n",
        "    transforms.Resize((64, 64)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
        "])\n",
        "\n",
        "class ChestXRayDataset(Dataset):\n",
        "    def __init__(self, csv_file, img_dir, transform=None):\n",
        "        self.data = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "        self.embed_dim = 512\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_name = os.path.join(self.img_dir, self.data.iloc[idx]['img'])\n",
        "        text = self.data.iloc[idx]['text']\n",
        "\n",
        "        image = Image.open(img_name).convert(\"RGB\")\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        text_vector = torch.zeros(self.embed_dim, dtype=torch.float32)\n",
        "        for i, char in enumerate(text[:self.embed_dim]):\n",
        "            text_vector[i] = ord(char) / 255.0\n",
        "\n",
        "        return image, text_vector, text\n",
        "\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self, embed_dim, num_heads, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attn = nn.MultiheadAttention(embed_dim, num_heads, batch_first=True)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(embed_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, embed_dim)\n",
        "        )\n",
        "        self.norm1 = nn.LayerNorm(embed_dim)\n",
        "        self.norm2 = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        attn_out, _ = self.attn(x, x, x)\n",
        "        x = self.norm1(x + attn_out)\n",
        "        fc_out = self.fc(x)\n",
        "        x = self.norm2(x + fc_out)\n",
        "        return x\n",
        "\n",
        "class VisionTransformerGenerator(nn.Module):\n",
        "    def __init__(self, embed_dim=512, img_size=64, patch_size=8, num_heads=8, num_layers=6, hidden_dim=1024):\n",
        "        super().__init__()\n",
        "        self.img_size = img_size\n",
        "        self.patch_size = patch_size\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.embed_dim = embed_dim\n",
        "\n",
        "        # Positional embedding for patches\n",
        "        self.pos_embedding = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
        "\n",
        "        # Transformer layers\n",
        "        self.encoder_layers = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim) for _ in range(num_layers)\n",
        "        ])\n",
        "\n",
        "        # Output projection to image patches\n",
        "        self.patch_projection = nn.Linear(embed_dim, patch_size * patch_size * 3)\n",
        "\n",
        "        # Final convolution to refine the output\n",
        "        self.final_conv = nn.Sequential(\n",
        "            nn.Conv2d(3, 3, kernel_size=3, padding=1),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: [batch_size, seq_len, embed_dim] from text encoder\n",
        "        B = x.size(0)\n",
        "        x = x[:, 0, :]  # Take the first (and only) sequence element: [batch_size, embed_dim]\n",
        "\n",
        "        # Expand text features to match number of patches\n",
        "        x = x.unsqueeze(1).repeat(1, self.num_patches, 1)  # [batch_size, num_patches, embed_dim]\n",
        "        x = x + self.pos_embedding.to(x.device)\n",
        "\n",
        "        # Process through transformer layers\n",
        "        for layer in self.encoder_layers:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Project to patch-sized image fragments\n",
        "        x = self.patch_projection(x)  # [batch_size, num_patches, patch_size*patch_size*3]\n",
        "\n",
        "        # Reshape to image format\n",
        "        x = x.view(B, self.num_patches, 3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 4, 1)  # [batch_size, 3, patch_size, patch_size, num_patches]\n",
        "\n",
        "        # Reshape patches into full image\n",
        "        grid_size = int(self.num_patches ** 0.5)\n",
        "        x = x.reshape(B, 3, self.patch_size, grid_size, self.patch_size, grid_size)\n",
        "        x = x.permute(0, 1, 3, 5, 2, 4).contiguous()\n",
        "        x = x.view(B, 3, self.img_size, self.img_size)\n",
        "\n",
        "        # Final refinement\n",
        "        x = self.final_conv(x)\n",
        "        return x\n",
        "\n",
        "class Text2ImageModel(nn.Module):\n",
        "    def __init__(self, embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=6, img_size=64):\n",
        "        super().__init__()\n",
        "        self.text_encoder = nn.ModuleList([\n",
        "            TransformerEncoderBlock(embed_dim, num_heads, hidden_dim)\n",
        "            for _ in range(num_layers)\n",
        "        ])\n",
        "        self.vit_generator = VisionTransformerGenerator(embed_dim, img_size)\n",
        "\n",
        "    def forward(self, text):\n",
        "        # Text encoding\n",
        "        x = text.unsqueeze(1)  # [batch_size, 1, embed_dim]\n",
        "        for layer in self.text_encoder:\n",
        "            x = layer(x)\n",
        "\n",
        "        # Generate image using ViT\n",
        "        generated_image = self.vit_generator(x)\n",
        "        return generated_image\n",
        "\n",
        "def train_model(model, dataloader, epochs=5, lr=0.0001):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.L1Loss()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "\n",
        "        for images, text_vectors, _ in dataloader:\n",
        "            images = images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            generated_images = model(text_vectors)\n",
        "            loss = criterion(generated_images, images)\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "        avg_loss = total_loss / len(dataloader)\n",
        "        print(f\"Epoch {epoch+1}/{epochs}, Train Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "def test_model(model, test_dataloader, save_dir=\"generated_images\", num_samples=8):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    criterion = nn.L1Loss()\n",
        "    total_loss = 0\n",
        "\n",
        "    if not os.path.exists(save_dir):\n",
        "        os.makedirs(save_dir)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (real_images, text_vectors, texts) in enumerate(test_dataloader):\n",
        "            if i >= 1:  # Process only first batch for visualization\n",
        "                break\n",
        "\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "\n",
        "            generated_images = model(text_vectors)\n",
        "\n",
        "            loss = criterion(generated_images, real_images)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            # Convert images to PIL for combining with text\n",
        "            num_to_save = min(num_samples, real_images.size(0))\n",
        "            real_images_np = real_images[:num_to_save].cpu().numpy().transpose(0, 2, 3, 1)\n",
        "            generated_images_np = generated_images[:num_to_save].cpu().numpy().transpose(0, 2, 3, 1)\n",
        "            texts_sample = texts[:num_to_save]\n",
        "\n",
        "            # Create side-by-side visualization for each sample\n",
        "            for idx in range(num_to_save):\n",
        "                real_img = Image.fromarray((real_images_np[idx] * 255).astype(np.uint8))\n",
        "                gen_img = Image.fromarray((generated_images_np[idx] * 255).astype(np.uint8))\n",
        "\n",
        "                text_img = Image.new('RGB', (200, 64), color=(255, 255, 255))\n",
        "                draw = ImageDraw.Draw(text_img)\n",
        "                try:\n",
        "                    font = ImageFont.truetype(\"arial.ttf\", 12)\n",
        "                except:\n",
        "                    font = ImageFont.load_default()\n",
        "                text = texts_sample[idx][:50] + \"...\" if len(texts_sample[idx]) > 50 else texts_sample[idx]\n",
        "                draw.text((5, 5), text, font=font, fill=(0, 0, 0))\n",
        "\n",
        "                combined_width = text_img.width + real_img.width + gen_img.width\n",
        "                combined_img = Image.new('RGB', (combined_width, 64))\n",
        "                combined_img.paste(text_img, (0, 0))\n",
        "                combined_img.paste(real_img, (text_img.width, 0))\n",
        "                combined_img.paste(gen_img, (text_img.width + real_img.width, 0))\n",
        "\n",
        "                combined_img.save(os.path.join(save_dir, f'sample_{idx}_text_real_gen.png'))\n",
        "\n",
        "            print(f\"Saved {num_to_save} samples (text | real | generated) to {save_dir}\")\n",
        "\n",
        "        # Calculate average loss across entire test set\n",
        "        for real_images, text_vectors, _ in test_dataloader:\n",
        "            real_images = real_images.to(device, dtype=torch.float32)\n",
        "            text_vectors = text_vectors.to(device, dtype=torch.float32)\n",
        "            generated_images = model(text_vectors)\n",
        "            total_loss += criterion(generated_images, real_images).item()\n",
        "\n",
        "    avg_loss = total_loss / len(test_dataloader)\n",
        "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    batch_size = 16\n",
        "    csv_path = \"/content/image_labels_reports.csv\"\n",
        "    extract_path = \"/content/image_dataset\"\n",
        "\n",
        "    full_dataset = ChestXRayDataset(csv_path, extract_path, transform=image_transforms)\n",
        "    train_size = int(0.8 * len(full_dataset))\n",
        "    test_size = len(full_dataset) - train_size\n",
        "    train_dataset, test_dataset = torch.utils.data.random_split(full_dataset, [train_size, test_size])\n",
        "\n",
        "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model = Text2ImageModel(embed_dim=512, hidden_dim=1024, num_heads=8, num_layers=2, img_size=64)\n",
        "    trained_model = train_model(model, train_dataloader, epochs=300)\n",
        "    test_model(trained_model, test_dataloader, num_samples=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aDsIRdaKO-lC",
        "outputId": "dedd39e4-1d97-4593-ebbb-3119160da21c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/300, Train Loss: 0.4654\n",
            "Epoch 2/300, Train Loss: 0.3869\n",
            "Epoch 3/300, Train Loss: 0.3544\n",
            "Epoch 4/300, Train Loss: 0.3412\n",
            "Epoch 5/300, Train Loss: 0.3368\n",
            "Epoch 6/300, Train Loss: 0.3343\n",
            "Epoch 7/300, Train Loss: 0.3341\n",
            "Epoch 8/300, Train Loss: 0.3323\n",
            "Epoch 9/300, Train Loss: 0.3308\n",
            "Epoch 10/300, Train Loss: 0.3318\n",
            "Epoch 11/300, Train Loss: 0.3310\n",
            "Epoch 12/300, Train Loss: 0.3299\n",
            "Epoch 13/300, Train Loss: 0.3290\n",
            "Epoch 14/300, Train Loss: 0.3304\n",
            "Epoch 15/300, Train Loss: 0.3289\n",
            "Epoch 16/300, Train Loss: 0.3293\n",
            "Epoch 17/300, Train Loss: 0.3292\n",
            "Epoch 18/300, Train Loss: 0.3290\n",
            "Epoch 19/300, Train Loss: 0.3288\n",
            "Epoch 20/300, Train Loss: 0.3274\n",
            "Epoch 21/300, Train Loss: 0.3272\n",
            "Epoch 22/300, Train Loss: 0.3278\n",
            "Epoch 23/300, Train Loss: 0.3269\n",
            "Epoch 24/300, Train Loss: 0.3251\n",
            "Epoch 25/300, Train Loss: 0.3252\n",
            "Epoch 26/300, Train Loss: 0.3257\n",
            "Epoch 27/300, Train Loss: 0.3250\n",
            "Epoch 28/300, Train Loss: 0.3243\n",
            "Epoch 29/300, Train Loss: 0.3250\n",
            "Epoch 30/300, Train Loss: 0.3245\n",
            "Epoch 31/300, Train Loss: 0.3238\n",
            "Epoch 32/300, Train Loss: 0.3247\n",
            "Epoch 33/300, Train Loss: 0.3246\n",
            "Epoch 34/300, Train Loss: 0.3240\n",
            "Epoch 35/300, Train Loss: 0.3229\n",
            "Epoch 36/300, Train Loss: 0.3196\n",
            "Epoch 37/300, Train Loss: 0.3232\n",
            "Epoch 38/300, Train Loss: 0.3220\n",
            "Epoch 39/300, Train Loss: 0.3198\n",
            "Epoch 40/300, Train Loss: 0.3151\n",
            "Epoch 41/300, Train Loss: 0.3096\n",
            "Epoch 42/300, Train Loss: 0.3124\n",
            "Epoch 43/300, Train Loss: 0.3071\n",
            "Epoch 44/300, Train Loss: 0.3038\n",
            "Epoch 45/300, Train Loss: 0.2989\n",
            "Epoch 46/300, Train Loss: 0.2965\n",
            "Epoch 47/300, Train Loss: 0.2899\n",
            "Epoch 48/300, Train Loss: 0.2868\n",
            "Epoch 49/300, Train Loss: 0.2851\n",
            "Epoch 50/300, Train Loss: 0.2810\n",
            "Epoch 51/300, Train Loss: 0.2820\n",
            "Epoch 52/300, Train Loss: 0.2781\n",
            "Epoch 53/300, Train Loss: 0.2741\n",
            "Epoch 54/300, Train Loss: 0.2704\n",
            "Epoch 55/300, Train Loss: 0.2694\n",
            "Epoch 56/300, Train Loss: 0.2673\n",
            "Epoch 57/300, Train Loss: 0.2639\n",
            "Epoch 58/300, Train Loss: 0.2619\n",
            "Epoch 59/300, Train Loss: 0.2608\n",
            "Epoch 60/300, Train Loss: 0.2593\n",
            "Epoch 61/300, Train Loss: 0.2581\n",
            "Epoch 62/300, Train Loss: 0.2565\n",
            "Epoch 63/300, Train Loss: 0.2559\n",
            "Epoch 64/300, Train Loss: 0.2557\n",
            "Epoch 65/300, Train Loss: 0.2538\n",
            "Epoch 66/300, Train Loss: 0.2528\n",
            "Epoch 67/300, Train Loss: 0.2509\n",
            "Epoch 68/300, Train Loss: 0.2502\n",
            "Epoch 69/300, Train Loss: 0.2498\n",
            "Epoch 70/300, Train Loss: 0.2501\n",
            "Epoch 71/300, Train Loss: 0.2477\n",
            "Epoch 72/300, Train Loss: 0.2475\n",
            "Epoch 73/300, Train Loss: 0.2472\n",
            "Epoch 74/300, Train Loss: 0.2449\n",
            "Epoch 75/300, Train Loss: 0.2460\n",
            "Epoch 76/300, Train Loss: 0.2443\n",
            "Epoch 77/300, Train Loss: 0.2441\n",
            "Epoch 78/300, Train Loss: 0.2445\n",
            "Epoch 79/300, Train Loss: 0.2438\n",
            "Epoch 80/300, Train Loss: 0.2426\n",
            "Epoch 81/300, Train Loss: 0.2412\n",
            "Epoch 82/300, Train Loss: 0.2417\n",
            "Epoch 83/300, Train Loss: 0.2402\n",
            "Epoch 84/300, Train Loss: 0.2416\n",
            "Epoch 85/300, Train Loss: 0.2415\n",
            "Epoch 86/300, Train Loss: 0.2411\n",
            "Epoch 87/300, Train Loss: 0.2397\n",
            "Epoch 88/300, Train Loss: 0.2397\n",
            "Epoch 89/300, Train Loss: 0.2385\n",
            "Epoch 90/300, Train Loss: 0.2386\n",
            "Epoch 91/300, Train Loss: 0.2368\n",
            "Epoch 92/300, Train Loss: 0.2365\n",
            "Epoch 93/300, Train Loss: 0.2359\n",
            "Epoch 94/300, Train Loss: 0.2365\n",
            "Epoch 95/300, Train Loss: 0.2368\n",
            "Epoch 96/300, Train Loss: 0.2364\n",
            "Epoch 97/300, Train Loss: 0.2363\n",
            "Epoch 98/300, Train Loss: 0.2366\n",
            "Epoch 99/300, Train Loss: 0.2358\n",
            "Epoch 100/300, Train Loss: 0.2364\n",
            "Epoch 101/300, Train Loss: 0.2361\n",
            "Epoch 102/300, Train Loss: 0.2360\n",
            "Epoch 103/300, Train Loss: 0.2348\n",
            "Epoch 104/300, Train Loss: 0.2355\n",
            "Epoch 105/300, Train Loss: 0.2347\n",
            "Epoch 106/300, Train Loss: 0.2334\n",
            "Epoch 107/300, Train Loss: 0.2344\n",
            "Epoch 108/300, Train Loss: 0.2329\n",
            "Epoch 109/300, Train Loss: 0.2336\n",
            "Epoch 110/300, Train Loss: 0.2323\n",
            "Epoch 111/300, Train Loss: 0.2348\n",
            "Epoch 112/300, Train Loss: 0.2315\n",
            "Epoch 113/300, Train Loss: 0.2326\n",
            "Epoch 114/300, Train Loss: 0.2323\n",
            "Epoch 115/300, Train Loss: 0.2321\n",
            "Epoch 116/300, Train Loss: 0.2308\n",
            "Epoch 117/300, Train Loss: 0.2317\n",
            "Epoch 118/300, Train Loss: 0.2310\n",
            "Epoch 119/300, Train Loss: 0.2308\n",
            "Epoch 120/300, Train Loss: 0.2314\n",
            "Epoch 121/300, Train Loss: 0.2325\n",
            "Epoch 122/300, Train Loss: 0.2324\n",
            "Epoch 123/300, Train Loss: 0.2315\n",
            "Epoch 124/300, Train Loss: 0.2311\n",
            "Epoch 125/300, Train Loss: 0.2308\n",
            "Epoch 126/300, Train Loss: 0.2313\n",
            "Epoch 127/300, Train Loss: 0.2297\n",
            "Epoch 128/300, Train Loss: 0.2295\n",
            "Epoch 129/300, Train Loss: 0.2302\n",
            "Epoch 130/300, Train Loss: 0.2295\n",
            "Epoch 131/300, Train Loss: 0.2295\n",
            "Epoch 132/300, Train Loss: 0.2290\n",
            "Epoch 133/300, Train Loss: 0.2287\n",
            "Epoch 134/300, Train Loss: 0.2287\n",
            "Epoch 135/300, Train Loss: 0.2295\n",
            "Epoch 136/300, Train Loss: 0.2293\n",
            "Epoch 137/300, Train Loss: 0.2296\n",
            "Epoch 138/300, Train Loss: 0.2288\n",
            "Epoch 139/300, Train Loss: 0.2280\n",
            "Epoch 140/300, Train Loss: 0.2293\n",
            "Epoch 141/300, Train Loss: 0.2293\n",
            "Epoch 142/300, Train Loss: 0.2281\n",
            "Epoch 143/300, Train Loss: 0.2294\n",
            "Epoch 144/300, Train Loss: 0.2281\n",
            "Epoch 145/300, Train Loss: 0.2290\n",
            "Epoch 146/300, Train Loss: 0.2285\n",
            "Epoch 147/300, Train Loss: 0.2288\n",
            "Epoch 148/300, Train Loss: 0.2274\n",
            "Epoch 149/300, Train Loss: 0.2266\n",
            "Epoch 150/300, Train Loss: 0.2289\n",
            "Epoch 151/300, Train Loss: 0.2282\n",
            "Epoch 152/300, Train Loss: 0.2269\n",
            "Epoch 153/300, Train Loss: 0.2275\n",
            "Epoch 154/300, Train Loss: 0.2272\n",
            "Epoch 155/300, Train Loss: 0.2281\n",
            "Epoch 156/300, Train Loss: 0.2269\n",
            "Epoch 157/300, Train Loss: 0.2265\n",
            "Epoch 158/300, Train Loss: 0.2264\n",
            "Epoch 159/300, Train Loss: 0.2263\n",
            "Epoch 160/300, Train Loss: 0.2263\n",
            "Epoch 161/300, Train Loss: 0.2257\n",
            "Epoch 162/300, Train Loss: 0.2274\n",
            "Epoch 163/300, Train Loss: 0.2259\n",
            "Epoch 164/300, Train Loss: 0.2257\n",
            "Epoch 165/300, Train Loss: 0.2257\n",
            "Epoch 166/300, Train Loss: 0.2249\n",
            "Epoch 167/300, Train Loss: 0.2255\n",
            "Epoch 168/300, Train Loss: 0.2266\n",
            "Epoch 169/300, Train Loss: 0.2264\n",
            "Epoch 170/300, Train Loss: 0.2269\n",
            "Epoch 171/300, Train Loss: 0.2259\n",
            "Epoch 172/300, Train Loss: 0.2259\n",
            "Epoch 173/300, Train Loss: 0.2240\n",
            "Epoch 174/300, Train Loss: 0.2252\n",
            "Epoch 175/300, Train Loss: 0.2259\n",
            "Epoch 176/300, Train Loss: 0.2258\n",
            "Epoch 177/300, Train Loss: 0.2254\n",
            "Epoch 178/300, Train Loss: 0.2253\n",
            "Epoch 179/300, Train Loss: 0.2250\n",
            "Epoch 180/300, Train Loss: 0.2245\n",
            "Epoch 181/300, Train Loss: 0.2257\n",
            "Epoch 182/300, Train Loss: 0.2247\n",
            "Epoch 183/300, Train Loss: 0.2234\n",
            "Epoch 184/300, Train Loss: 0.2240\n",
            "Epoch 185/300, Train Loss: 0.2229\n",
            "Epoch 186/300, Train Loss: 0.2249\n",
            "Epoch 187/300, Train Loss: 0.2240\n",
            "Epoch 188/300, Train Loss: 0.2253\n",
            "Epoch 189/300, Train Loss: 0.2238\n",
            "Epoch 190/300, Train Loss: 0.2245\n",
            "Epoch 191/300, Train Loss: 0.2242\n",
            "Epoch 192/300, Train Loss: 0.2235\n",
            "Epoch 193/300, Train Loss: 0.2229\n",
            "Epoch 194/300, Train Loss: 0.2225\n",
            "Epoch 195/300, Train Loss: 0.2231\n",
            "Epoch 196/300, Train Loss: 0.2226\n",
            "Epoch 197/300, Train Loss: 0.2222\n",
            "Epoch 198/300, Train Loss: 0.2236\n",
            "Epoch 199/300, Train Loss: 0.2236\n",
            "Epoch 200/300, Train Loss: 0.2241\n",
            "Epoch 201/300, Train Loss: 0.2225\n",
            "Epoch 202/300, Train Loss: 0.2231\n",
            "Epoch 203/300, Train Loss: 0.2223\n",
            "Epoch 204/300, Train Loss: 0.2229\n",
            "Epoch 205/300, Train Loss: 0.2234\n",
            "Epoch 206/300, Train Loss: 0.2221\n",
            "Epoch 207/300, Train Loss: 0.2231\n",
            "Epoch 208/300, Train Loss: 0.2223\n",
            "Epoch 209/300, Train Loss: 0.2235\n",
            "Epoch 210/300, Train Loss: 0.2222\n",
            "Epoch 211/300, Train Loss: 0.2228\n",
            "Epoch 212/300, Train Loss: 0.2228\n",
            "Epoch 213/300, Train Loss: 0.2228\n",
            "Epoch 214/300, Train Loss: 0.2229\n",
            "Epoch 215/300, Train Loss: 0.2226\n",
            "Epoch 216/300, Train Loss: 0.2231\n",
            "Epoch 217/300, Train Loss: 0.2232\n",
            "Epoch 218/300, Train Loss: 0.2208\n",
            "Epoch 219/300, Train Loss: 0.2217\n",
            "Epoch 220/300, Train Loss: 0.2216\n",
            "Epoch 221/300, Train Loss: 0.2214\n",
            "Epoch 222/300, Train Loss: 0.2217\n",
            "Epoch 223/300, Train Loss: 0.2210\n",
            "Epoch 224/300, Train Loss: 0.2206\n",
            "Epoch 225/300, Train Loss: 0.2229\n",
            "Epoch 226/300, Train Loss: 0.2221\n",
            "Epoch 227/300, Train Loss: 0.2216\n",
            "Epoch 228/300, Train Loss: 0.2215\n",
            "Epoch 229/300, Train Loss: 0.2210\n",
            "Epoch 230/300, Train Loss: 0.2217\n",
            "Epoch 231/300, Train Loss: 0.2216\n",
            "Epoch 232/300, Train Loss: 0.2222\n",
            "Epoch 233/300, Train Loss: 0.2207\n",
            "Epoch 234/300, Train Loss: 0.2205\n",
            "Epoch 235/300, Train Loss: 0.2212\n",
            "Epoch 236/300, Train Loss: 0.2217\n",
            "Epoch 237/300, Train Loss: 0.2208\n",
            "Epoch 238/300, Train Loss: 0.2202\n",
            "Epoch 239/300, Train Loss: 0.2204\n",
            "Epoch 240/300, Train Loss: 0.2214\n",
            "Epoch 241/300, Train Loss: 0.2203\n",
            "Epoch 242/300, Train Loss: 0.2209\n",
            "Epoch 243/300, Train Loss: 0.2204\n",
            "Epoch 244/300, Train Loss: 0.2196\n",
            "Epoch 245/300, Train Loss: 0.2204\n",
            "Epoch 246/300, Train Loss: 0.2202\n",
            "Epoch 247/300, Train Loss: 0.2200\n",
            "Epoch 248/300, Train Loss: 0.2206\n",
            "Epoch 249/300, Train Loss: 0.2194\n",
            "Epoch 250/300, Train Loss: 0.2209\n",
            "Epoch 251/300, Train Loss: 0.2211\n",
            "Epoch 252/300, Train Loss: 0.2202\n",
            "Epoch 253/300, Train Loss: 0.2192\n",
            "Epoch 254/300, Train Loss: 0.2200\n",
            "Epoch 255/300, Train Loss: 0.2213\n",
            "Epoch 256/300, Train Loss: 0.2203\n",
            "Epoch 257/300, Train Loss: 0.2202\n",
            "Epoch 258/300, Train Loss: 0.2200\n",
            "Epoch 259/300, Train Loss: 0.2193\n",
            "Epoch 260/300, Train Loss: 0.2195\n",
            "Epoch 261/300, Train Loss: 0.2197\n",
            "Epoch 262/300, Train Loss: 0.2190\n",
            "Epoch 263/300, Train Loss: 0.2199\n",
            "Epoch 264/300, Train Loss: 0.2198\n",
            "Epoch 265/300, Train Loss: 0.2191\n",
            "Epoch 266/300, Train Loss: 0.2203\n",
            "Epoch 267/300, Train Loss: 0.2195\n",
            "Epoch 268/300, Train Loss: 0.2199\n",
            "Epoch 269/300, Train Loss: 0.2190\n",
            "Epoch 270/300, Train Loss: 0.2192\n",
            "Epoch 271/300, Train Loss: 0.2196\n",
            "Epoch 272/300, Train Loss: 0.2198\n",
            "Epoch 273/300, Train Loss: 0.2199\n",
            "Epoch 274/300, Train Loss: 0.2192\n",
            "Epoch 275/300, Train Loss: 0.2190\n",
            "Epoch 276/300, Train Loss: 0.2193\n",
            "Epoch 277/300, Train Loss: 0.2195\n",
            "Epoch 278/300, Train Loss: 0.2196\n",
            "Epoch 279/300, Train Loss: 0.2191\n",
            "Epoch 280/300, Train Loss: 0.2183\n",
            "Epoch 281/300, Train Loss: 0.2192\n",
            "Epoch 282/300, Train Loss: 0.2188\n",
            "Epoch 283/300, Train Loss: 0.2193\n",
            "Epoch 284/300, Train Loss: 0.2186\n",
            "Epoch 285/300, Train Loss: 0.2189\n",
            "Epoch 286/300, Train Loss: 0.2181\n",
            "Epoch 287/300, Train Loss: 0.2185\n",
            "Epoch 288/300, Train Loss: 0.2187\n",
            "Epoch 289/300, Train Loss: 0.2182\n",
            "Epoch 290/300, Train Loss: 0.2192\n",
            "Epoch 291/300, Train Loss: 0.2188\n",
            "Epoch 292/300, Train Loss: 0.2182\n",
            "Epoch 293/300, Train Loss: 0.2180\n",
            "Epoch 294/300, Train Loss: 0.2185\n",
            "Epoch 295/300, Train Loss: 0.2180\n",
            "Epoch 296/300, Train Loss: 0.2192\n",
            "Epoch 297/300, Train Loss: 0.2189\n",
            "Epoch 298/300, Train Loss: 0.2181\n",
            "Epoch 299/300, Train Loss: 0.2190\n",
            "Epoch 300/300, Train Loss: 0.2178\n",
            "Saved 2 samples (text | real | generated) to generated_images\n",
            "Test Loss: 0.3761\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm -rf /content/generated_images"
      ],
      "metadata": {
        "id": "BJLODkNrO--L"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0z3fXuZ7QdVt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}